{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blurred421/LFD473-code/blob/main/labs/jb_Solved%20Lab%204.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BsKDsa5LS9D"
      },
      "source": [
        "# Lab Instructions\n",
        "\n",
        "In the lab, you're presented a task such as building a dataset, training a model, or writing a training loop, and we'll provide the code structured in such a way that you can fill in the blanks in the code using the knowledge you acquired in the chapters that precede the lab. You should be able to find appropriate snippets of code in the course content that work well in the lab with minor or no adjustments.\n",
        "\n",
        "The blanks in the code are indicated by ellipsis (`...`) and comments (`# write your code here`).\n",
        "\n",
        "In some cases, we'll provide you partial code to ensure the right variables are populated and any code that follows it runs accordingly.\n",
        "\n",
        "```python\n",
        "# write your code here\n",
        "x = ...\n",
        "```\n",
        "\n",
        "The solution should be a single statement that replaces the ellipsis, such as:\n",
        "\n",
        "```python\n",
        "# write your code here\n",
        "x = [0, 1, 2]\n",
        "```\n",
        "\n",
        "In some other cases, when there is no new variable being created, the blanks are shown like in the example below:\n",
        "\n",
        "```python\n",
        "# write your code here\n",
        "...\n",
        "```\n",
        "\n",
        "Although we're showing you only a single ellipsis (`...`), you may have to write more than one line of code to complete the step, such as:\n",
        "\n",
        "```python\n",
        "# write your code here\n",
        "for i, xi in enumerate(x):\n",
        "    x[i] = xi * 2\n",
        "```"
      ],
      "id": "-BsKDsa5LS9D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjQj9A4pLS9F"
      },
      "source": [
        "## Installation Notes\n",
        "\n",
        "To run this notebook on Google Colab, you will need to install the following library: datasets.\n",
        "\n",
        "In Google Colab, you can run the following command to install this library:"
      ],
      "id": "GjQj9A4pLS9F"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-nMEgglLS9G"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ],
      "id": "i-nMEgglLS9G"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b17bded5"
      },
      "source": [
        "## 8.5 Lab 4: Sentiment Analysis"
      ],
      "id": "b17bded5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d1bb429"
      },
      "source": [
        "In this lab, you'll fine-tune an encoder-based model to perform sentiment analysis on the Standford Sentiment Treebank (SST2) dataset. You'll load RoBERTa's sibling, XLM-RoBERTa, use its prescribed transformations to preprocess text in the SST2 dataset, and fine-tune (train) it for one epoch."
      ],
      "id": "4d1bb429"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "585fd02c"
      },
      "source": [
        "### 8.5.1 Model\n",
        "\n",
        "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step1.png)\n",
        "\n",
        "You'll use Hugging Face's `XLMRobertaForSequenceClassification` to perform binary classification (we have two classes, \"positive\" and \"negative\" sentiment)."
      ],
      "id": "585fd02c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6453fb87"
      },
      "outputs": [],
      "source": [
        "from transformers import XLMRobertaForSequenceClassification\n",
        "repo_id = \"FacebookAI/xlm-roberta-base\"\n",
        "\n",
        "model = XLMRobertaForSequenceClassification.from_pretrained(repo_id, num_labels=2)\n",
        "model"
      ],
      "id": "6453fb87"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52f10c74"
      },
      "source": [
        "### 8.5.2 Dataset\n",
        "\n",
        "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step1.png)\n",
        "\n",
        "Now, you will load Hugging Face's [\"Stanford Sentiment Treebank (SST2)\"](https://huggingface.co/datasets/stanfordnlp/sst2) dataset. It is already split into `train`, `validation`, and `test` sets."
      ],
      "id": "52f10c74"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65b37a60"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "datasets = load_dataset('stanfordnlp/sst2')\n",
        "datasets"
      ],
      "id": "65b37a60"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f933d20a"
      },
      "source": [
        "Let's take a look at one data point from the SST2 dataset. Just run the code below as is to visualize the output:"
      ],
      "id": "f933d20a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d15fa20"
      },
      "outputs": [],
      "source": [
        "row = datasets['train'][0]\n",
        "text, label = row['sentence'], row['label']\n",
        "text, label"
      ],
      "id": "3d15fa20"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "377fe053"
      },
      "source": [
        "Each data point is a dictionary, containing a line of text, and the corresponding label - the sentiment (0 for negative, 1 for positive)."
      ],
      "id": "377fe053"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eebde7e"
      },
      "source": [
        "### 8.5.3 Tokenizer\n",
        "\n",
        "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step3.png)\n",
        "\n",
        "You already know the drill: you must preprocess the input (the text) using the prescribed transformation for the model you're using, so it gets tokenized, converted into token ids, and prependend/appended with the appropriate special tokens.\n",
        "\n",
        "Load XLM-RoBERTa's tokenizer and write a function that takes a dictionary with the `sentence` key (it may have other keys as well) and returns a dictionary with `input_ids`, `attention_mask` keys (remember that the `map()` method of HF datasets work by _merging_ dictionaries):"
      ],
      "id": "6eebde7e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c217192"
      },
      "outputs": [],
      "source": [
        "from transformers import XLMRobertaTokenizer\n",
        "\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained(repo_id)"
      ],
      "id": "0c217192"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f18d864c"
      },
      "outputs": [],
      "source": [
        "def apply_transform(row):\n",
        "    text = row['sentence']\n",
        "    # Use the transform_fn you retrieved in the previous cell to\n",
        "    # preprocess the text\n",
        "    # write your code here\n",
        "    return tokenizer(text)"
      ],
      "id": "f18d864c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcd58738"
      },
      "source": [
        "Let's apply your function to our data point to see if it is working as expected (just run the code below as is to visualize the output):"
      ],
      "id": "dcd58738"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ca73f45"
      },
      "outputs": [],
      "source": [
        "apply_transform(row)"
      ],
      "id": "6ca73f45"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlQPes_WLS9Z"
      },
      "source": [
        "Now, apply the function to every row in our datasets:"
      ],
      "id": "TlQPes_WLS9Z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Q6yFG-oLS9a"
      },
      "outputs": [],
      "source": [
        "datasets = datasets.map(apply_transform)\n",
        "datasets"
      ],
      "id": "6Q6yFG-oLS9a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjHX6wEiLS9b"
      },
      "source": [
        "To keep our datasets tidy, selct only the columns we're interested in (`input_ids` and `label`):"
      ],
      "id": "kjHX6wEiLS9b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ap93fqF2LS9c"
      },
      "outputs": [],
      "source": [
        "datasets = datasets.select_columns(['input_ids', 'label'])\n",
        "datasets['train'][:3]"
      ],
      "id": "Ap93fqF2LS9c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32ec29d1"
      },
      "source": [
        "Did you notice the transformation is returning a regular Python list of token ids, not a PyTorch tensor? Remember, we cannot make a tensor out of lists of different lengths (see section 6.3.3). The solution? Padding the shorter sentences, so they all have the same length.\n",
        "\n",
        "But, how can we think of padding sentences if we don't have a mini-batch yet? We delegate this job to the dataloaders's collate function!"
      ],
      "id": "32ec29d1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d570206"
      },
      "source": [
        "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step5.png)\n",
        "\n",
        "So far, we've been using data loaders without specifying a collate function, that is, we're using its default collate function. For tabular data, the default collator is more than enough. It simply stacks several data points together and, since they all have the same size, it works smoothly. But this strategy breaks apart when we're dealing with sequences of different lengths, as we've already experienced while trying to make a tensor out of them.\n",
        "\n",
        "Just like before, padding is the solution for our problem, and we're using a [collator](https://huggingface.co/learn/nlp-course/en/chapter3/2#dynamic-padding) designed to automatically pad the sequences before stacking them together: HF's `DataCollatorWithPadding`. It takes the tokenizer as an argument in order to determine which token is the padding token, and which side (left or right) should be padded.\n",
        "\n",
        "Let's try it on a slice of four sequences from our training set:"
      ],
      "id": "0d570206"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2G60AxHGLS9f"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "data_collator(datasets['train'][:4])"
      ],
      "id": "2G60AxHGLS9f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHFLtkaXLS9g"
      },
      "source": [
        "You can easily recognize the padding tokens sitting at the right end of the sequences (a sequence of ones). Moreover, the location of every padding token is indicated by the sequence's corresponding attention mask. The masks tell the model which tokens should be considered (value of one) or ignored (value of zero).\n",
        "\n",
        "Next, let's assign this data collator to each dataloader:"
      ],
      "id": "yHFLtkaXLS9g"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76c804f3"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataloaders = {}\n",
        "# write your code here\n",
        "dataloaders['train'] = DataLoader(dataset=datasets['train'], batch_size=16, shuffle=True, collate_fn=data_collator)\n",
        "dataloaders['val'] = DataLoader(dataset=datasets['validation'], batch_size=16, shuffle=True, collate_fn=data_collator)"
      ],
      "id": "76c804f3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "923f3846"
      },
      "source": [
        "Now, let's fetch a mini-batch from our data loader (just run the code below as is to visualize the output):"
      ],
      "id": "923f3846"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05babc84"
      },
      "outputs": [],
      "source": [
        "dl_out = next(iter(dataloaders['train']))\n",
        "dl_out"
      ],
      "id": "05babc84"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4eb7806"
      },
      "source": [
        "As you can see, there are plenty of padding tokens there. The collator will always pad the sequences to match the longest sequence in a particular mini-batch. This means that mini-batches may have sequences of different lengths (when compared across mini-batches but not inside the same one)."
      ],
      "id": "d4eb7806"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "171714e6"
      },
      "source": [
        "### 8.5.4 Training"
      ],
      "id": "171714e6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "260d405e"
      },
      "source": [
        "Now, it is time to write a training loop to fine-tune your XLM-RoBERTa model on the SST2 dataset. This is a large model, and the training set has over 60,000 data points, so you can train it over a single epoch, that is, looping over the mini-batches from the datapipe (or data loader) only once. For the sake of speed, keep the evalution for the end only."
      ],
      "id": "260d405e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A98-FXiLS9s"
      },
      "source": [
        "#### 8.5.4.1 Loss Function\n",
        "\n",
        "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step2.png)\n",
        "\n",
        "Sentiment analysis is a classification task, so we need to use the appropriate loss function for the task. Even though it is a binary classification, RoBERTa's classification head is actually producing two logits instead of one, so you have to use `CrossEntropyLoss` (which can handle two or more logits)."
      ],
      "id": "9A98-FXiLS9s"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "330c949d"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "id": "330c949d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylpU39c4LS-E"
      },
      "source": [
        "This step is actually redundant now. Since we're using a HF model, the loss is automatically returned when the model is in training model. We simply retrieve the loss from the output's `loss` attribute."
      ],
      "id": "ylpU39c4LS-E"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiMz78USLS-F"
      },
      "source": [
        "#### 8.5.4.2 Optimizer\n",
        "\n",
        "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step3.png)"
      ],
      "id": "LiMz78USLS-F"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfDSj2Z0LS-F"
      },
      "source": [
        "Although `Adam` is the optimizer of choice, we suggest you try out `AdamW`, a modified version that is also commonly used."
      ],
      "id": "UfDSj2Z0LS-F"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXHNG8wvLS-G"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# suggested learning rate\n",
        "lr = 1e-5\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=lr)"
      ],
      "id": "FXHNG8wvLS-G"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvixsnlYLS-H"
      },
      "source": [
        "#### 8.4.4.2 Training Loop"
      ],
      "id": "fvixsnlYLS-H"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YveaXzXhLS-I"
      },
      "source": [
        "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step4.png)\n",
        "\n",
        "So far, we haven't logged or inspected our losses in real-time. Why bother, if it takes only a minute to train the model? This time is different, though: fine-tuning RoBERTa on more than 67,000 data points, even for a single epoch, will take about 15 min or so in Google Colab. So, let's use TensorBoard to see how our loss is doing as training progresses.\n",
        "\n",
        "First, we need to load it using the corresponding Jupyter magic (just run the code below as is to load TensorBoard):"
      ],
      "id": "YveaXzXhLS-I"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWc73L7FLS-V"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ],
      "id": "vWc73L7FLS-V"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVEgrpL0LS-W"
      },
      "source": [
        "Next, we need to create an instance of the `SummaryWriter` to be able to send loss values to TensorBoard. Just run the code below as is to create it:"
      ],
      "id": "OVEgrpL0LS-W"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FBbWMnLLS-X"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter('runs/roberta')"
      ],
      "id": "8FBbWMnLLS-X"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJ1Sem-1LS-X"
      },
      "source": [
        "Now, it's your turn to write the missing parts of the training loop below. We have already taken care of the sending the losses to TensorBoard for you."
      ],
      "id": "pJ1Sem-1LS-X"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7b75295"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "batch_losses = []\n",
        "\n",
        "## Training\n",
        "for i, batch in tqdm(enumerate(dataloaders['train'])):\n",
        "    batch_features = batch['input_ids']\n",
        "    batch_targets = batch['labels']\n",
        "    batch_masks = batch['attention_mask']\n",
        "    # Set the model's mode\n",
        "    # write your code here\n",
        "    model.train()\n",
        "\n",
        "    # Send input_ids, labels, and attention masks to the device\n",
        "    # write your code here\n",
        "    batch_features = batch_features.to(device)\n",
        "    batch_targets = batch_targets.to(device)\n",
        "    batch_masks = batch_masks.to(device)\n",
        "\n",
        "    # Step 1 - forward pass\n",
        "    # write your code here\n",
        "    output = model(input_ids=batch_features,\n",
        "                        attention_mask=batch_masks,\n",
        "                        labels=batch_targets)\n",
        "    prediction = output.logits\n",
        "\n",
        "    # Step 2 - computing the loss\n",
        "    # write your code here\n",
        "    loss = output.loss\n",
        "\n",
        "    # Step 3 - computing the gradients\n",
        "    # Tip: it requires a single method call to backpropagate gradients\n",
        "    # write your code here\n",
        "    loss.backward()\n",
        "\n",
        "    batch_losses.append(loss.item())\n",
        "\n",
        "    writer.add_scalars(main_tag='loss',\n",
        "                       tag_scalar_dict={'training': loss.item()},\n",
        "                       global_step=i)\n",
        "\n",
        "    # Step 4 - updating parameters and zeroing gradients\n",
        "    # Tip: it takes two calls to optimizer's methods\n",
        "    # write your code here\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "writer.close()\n",
        "\n",
        "## Validation\n",
        "with torch.inference_mode():\n",
        "    val_losses = []\n",
        "\n",
        "    #for i, (val_features, val_targets) in enumerate(dataloaders['val']):\n",
        "    for i, val in enumerate(dataloaders['val']):\n",
        "        val_features = val['input_ids']\n",
        "        val_targets = val['labels']\n",
        "        val_masks = val['attention_mask']\n",
        "        # Set the model's mode\n",
        "        # write your code here\n",
        "        model.eval()\n",
        "\n",
        "        # Send input_ids, labels, and attention masks to the device\n",
        "        # write your code here\n",
        "        val_features = val_features.to(device)\n",
        "        val_targets = val_targets.to(device)\n",
        "        val_masks = val_masks.to(device)\n",
        "\n",
        "        # Step 1 - forward pass\n",
        "        # write your code here\n",
        "        output = model(input_ids=val_features, attention_mask=val_masks, labels=val_targets)\n",
        "        predictions = output.logits\n",
        "\n",
        "        # Step 2 - computing the loss\n",
        "        # write your code here\n",
        "        loss = output.loss\n",
        "\n",
        "        val_losses.append(loss.item())"
      ],
      "id": "a7b75295"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVlQkNwwLS-Y"
      },
      "source": [
        "By the end of it, your losses on TensorBoard should look more or less like this (if you drag the slider on the right to the maximum level of smoothing):\n",
        "\n",
        "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch6/tensorboard.png)"
      ],
      "id": "uVlQkNwwLS-Y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "724660c3"
      },
      "source": [
        "### 8.5.5 Inference\n",
        "\n",
        "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step5.png)\n",
        "\n",
        "Write a function that takes some text (a sequence of words), a model, its tokenizer, and a list of target categories for the classification, and returns the most likely category and the corresponding probability.\n",
        "\n",
        "Since you're handling a single sequence, there's no need for any padding, but you still need to provide a tensor containing a mini-batch (of one) as input to the model.\n",
        "\n",
        "The model returns two logits, one for each class, so you must use the softmax function to convert them into probabilities."
      ],
      "id": "724660c3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9a8d274"
      },
      "outputs": [],
      "source": [
        "def predict(sequence, model, tokenizer, categories):\n",
        "    # Build a tensor of token ids out of the input sequence\n",
        "    # write your code here\n",
        "    token_ids = tokenizer(sequence, return_tensors='pt')['input_ids']\n",
        "\n",
        "    # Set the model to the appropriate mode\n",
        "    # write your code here\n",
        "    model.eval()\n",
        "\n",
        "    device = next(iter(model.parameters())).device\n",
        "\n",
        "    # Use the model to make predictions/logits\n",
        "    # Tip: Don't forget to send the input to the same device as the model\n",
        "    # write your code here\n",
        "    pred = model(token_ids.to(device)).logits\n",
        "\n",
        "    # Compute the probabilities corresponding to the logits\n",
        "    # and return the top value and index\n",
        "    # write your code here\n",
        "    probabilities = torch.nn.functional.softmax(pred[0], dim=0)\n",
        "    values, indices = torch.topk(probabilities, 1)\n",
        "\n",
        "    return [{'label': categories[i], 'value': v.item()} for i, v in zip(indices, values)]"
      ],
      "id": "a9a8d274"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b587f62f"
      },
      "source": [
        "Now, try out your prediction function and fine-tuned model (just run the code cells below as they are to visualize their outputs):"
      ],
      "id": "b587f62f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe87a17e"
      },
      "outputs": [],
      "source": [
        "categories = ['negative', 'positive']\n",
        "text = \"I am really liking this course\"\n",
        "predict(text, model, tokenizer, categories)"
      ],
      "id": "fe87a17e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7b50f0c7"
      },
      "outputs": [],
      "source": [
        "text = \"This course is too complicated!\"\n",
        "predict(text, model, tokenizer, categories)"
      ],
      "id": "7b50f0c7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "578f344d"
      },
      "source": [
        "That's cool, but what if we could perform sentiment analysis out-of-the-box? That's what we'll do in the second part of Chapter 6."
      ],
      "id": "578f344d"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
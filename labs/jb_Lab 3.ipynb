{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blurred421/LFD473-code/blob/main/labs/jb_Lab%203.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67609b90",
      "metadata": {
        "id": "67609b90"
      },
      "source": [
        "# Lab Instructions\n",
        "\n",
        "In the lab, you're presented a task such as building a dataset, training a model, or writing a training loop, and we'll provide the code structured in such a way that you can fill in the blanks in the code using the knowledge you acquired in the chapters that precede the lab. You should be able to find appropriate snippets of code in the course content that work well in the lab with minor or no adjustments.\n",
        "\n",
        "The blanks in the code are indicated by ellipsis (`...`) and comments (`# write your code here`).\n",
        "\n",
        "In some cases, we'll provide you partial code to ensure the right variables are populated and any code that follows it runs accordingly.\n",
        "\n",
        "```python\n",
        "# write your code here\n",
        "x = ...\n",
        "```\n",
        "\n",
        "The solution should be a single statement that replaces the ellipsis, such as:\n",
        "\n",
        "```python\n",
        "# write your code here\n",
        "x = [0, 1, 2]\n",
        "```\n",
        "\n",
        "In some other cases, when there is no new variable being created, the blanks are shown like in the example below:\n",
        "\n",
        "```python\n",
        "# write your code here\n",
        "...\n",
        "```\n",
        "\n",
        "Although we're showing you only a single ellipsis (`...`), you may have to write more than one line of code to complete the step, such as:\n",
        "\n",
        "```python\n",
        "# write your code here\n",
        "for i, xi in enumerate(x):\n",
        "    x[i] = xi * 2\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da35abb9",
      "metadata": {
        "id": "da35abb9"
      },
      "source": [
        "## 7.8 Lab 3: Classifying Images\n",
        "\n",
        "Now it is YOUR turn to classify some images! First, you will need to choose and load a [model for image classification](https://pytorch.org/vision/stable/models.html#classification) and its corresponding [weights](https://pytorch.org/vision/stable/models.html#table-of-all-available-classification-weights).\n",
        "\n",
        "Don't forget to retrieve the prescribed transformation function or model corresponding to the model you chose. Also, take a look at its size and accuracy, so you have an idea of its performance.\n",
        "\n",
        "TIP: try a very small model (e.g. MobileNet) and a very large model (e.g. VGG) and see how long they take to run inference on your images."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7767b765",
      "metadata": {
        "id": "7767b765"
      },
      "source": [
        "### 7.8.1 Load Weights\n",
        "\n",
        "Load the weights from the model of your choice into its own object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab6ed969",
      "metadata": {
        "id": "ab6ed969"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import get_weight\n",
        "\n",
        "# write your code here\n",
        "weights = get_weight('MobileNet_V3_Small_Weights.DEFAULT')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4512d16",
      "metadata": {
        "id": "f4512d16"
      },
      "source": [
        "### 7.8.2 Load Model\n",
        "\n",
        "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step1.png)\n",
        "\n",
        "Load the model using Torch Hub and the weights you've just loaded:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24757d81",
      "metadata": {
        "id": "24757d81"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "repo = 'pytorch/vision'\n",
        "\n",
        "# write your code here\n",
        "model = torch.hub.load(repo, 'mobilenet_v3_small', weights=weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f8b8660",
      "metadata": {
        "id": "7f8b8660"
      },
      "source": [
        "### 7.8.3 Extract Metadata\n",
        "\n",
        "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step3.png)\n",
        "\n",
        "Retrieve the categories used to pretrain the model, and the transformation function that should be applied to the input images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69d7aa03",
      "metadata": {
        "id": "69d7aa03"
      },
      "outputs": [],
      "source": [
        "# write your code here\n",
        "categories = weights.meta['categories']\n",
        "transforms_fn = weights.transforms()\n",
        "\n",
        "transforms_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fad143b",
      "metadata": {
        "id": "3fad143b"
      },
      "source": [
        "Let's inspect the number of parameters and the metrics of the model you chose. Run the two cells below as they are to visualize their output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b38912d",
      "metadata": {
        "id": "9b38912d"
      },
      "outputs": [],
      "source": [
        "weights.meta['num_params']/1e6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02e30ccc",
      "metadata": {
        "id": "02e30ccc"
      },
      "outputs": [],
      "source": [
        "weights.meta['_metrics']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a870b00f",
      "metadata": {
        "id": "a870b00f"
      },
      "source": [
        "### 7.8.4 Making Predictions\n",
        "\n",
        "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step5.png)\n",
        "\n",
        "Now, let's use the pretrained model you've already loaded to make predictions for an image. First, though, let's download an image from Wikipedia once again. The downloading function is reproduced below for convenience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9e5d88b",
      "metadata": {
        "id": "c9e5d88b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "def get_image_from_url(url, headers=None):\n",
        "    if headers is None:\n",
        "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response.raise_for_status()\n",
        "    img = Image.open(BytesIO(response.content))\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1720756",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1720756",
        "outputId": "331e0188-f627-460a-860d-e15b3c61a77d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(846, 1075)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "url = 'https://upload.wikimedia.org/wikipedia/commons/7/72/Igel.JPG'\n",
        "img = get_image_from_url(url)\n",
        "img.height, img.width"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad094453",
      "metadata": {
        "id": "ad094453"
      },
      "source": [
        "Remember, the model cannot take PIL images as inputs, you need to preprocess the image first:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d25ae65",
      "metadata": {
        "id": "8d25ae65"
      },
      "outputs": [],
      "source": [
        "# write your code here\n",
        "preprocessed_img = transforms_fn(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f01cdc8e",
      "metadata": {
        "id": "f01cdc8e"
      },
      "source": [
        "Moreover, models expect mini-batches, not single images as inputs. Make sure you have a mini-batch with the right shape (N, C, H, W), standing for number of images in the mini-batch (one, in our case), number of channels in the image, its height, and its width:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83438a80",
      "metadata": {
        "id": "83438a80"
      },
      "outputs": [],
      "source": [
        "# write your code here\n",
        "mini_batch = preprocessed_img.unsqueeze(0)\n",
        "mini_batch.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc624901",
      "metadata": {
        "id": "fc624901"
      },
      "source": [
        "You can use the mini-batch and the pretrained model to make predictions now:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f7cfdae",
      "metadata": {
        "id": "9f7cfdae"
      },
      "outputs": [],
      "source": [
        "# The mini-batch above has a single data point\n",
        "# Call the model and get the corresponding predictions(logits)\n",
        "# write your code here\n",
        "logit = model(mini_batch)[0]\n",
        "\n",
        "# Fetch the index of the highest logit\n",
        "# write your code here\n",
        "idx = logit.argmax()\n",
        "\n",
        "# Find the corresponding category\n",
        "categories[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfb53b99",
      "metadata": {
        "id": "dfb53b99"
      },
      "source": [
        "Perhaps you've figured it out that we forgot to set the model to evaluation mode. That shoudl fix it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea7ea6cb",
      "metadata": {
        "id": "ea7ea6cb"
      },
      "outputs": [],
      "source": [
        "# Set the model to evaluation mode\n",
        "# write your code here\n",
        "model.eval()\n",
        "\n",
        "\n",
        "# Then find the predicted category as above\n",
        "# write your code here\n",
        "logit = model(mini_batch)[0]\n",
        "idx = logit.argmax()\n",
        "categories[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f884177",
      "metadata": {
        "id": "5f884177"
      },
      "source": [
        "#### 7.8.4.1 Probabilities\n",
        "\n",
        "In many cases, it may be interesting to return the probabilities next to the predictions. Convert the logits produced by the model into probabilities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51a457d9",
      "metadata": {
        "id": "51a457d9"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# write your code here\n",
        "probabilities = F.softmax(logit, dim=0)\n",
        "probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "759c9173",
      "metadata": {
        "id": "759c9173"
      },
      "source": [
        "Use PyTorch's own built-in function to get the top-K probabilities and corresponding indices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16d3b6dc",
      "metadata": {
        "id": "16d3b6dc"
      },
      "outputs": [],
      "source": [
        "# write your code here\n",
        "values, indices = torch.topk(probabilities, 1)\n",
        "values, indices"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea38af6f",
      "metadata": {
        "id": "ea38af6f"
      },
      "source": [
        "The target or label is the class corresponding to the index above. Just run the cell below as is to visualize its output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6435319e",
      "metadata": {
        "id": "6435319e"
      },
      "outputs": [],
      "source": [
        "categories[indices[0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d26d132",
      "metadata": {
        "id": "0d26d132"
      },
      "source": [
        "#### 7.8.4.2 Testing\n",
        "\n",
        "In a real-world deployment, you won't have the input data neatly assembled as a dataset. You will have to create a mini-batch of the user's input data, feed it to the model to get its predicted logits, and then convert them into one or more predictions and probabilities that need to be returned to the user.\n",
        "\n",
        "Write a function that takes either an URL or a filepath, a model, its prescribed transformations, and a list of target categories, and returns a list of the top K predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8419346",
      "metadata": {
        "id": "d8419346"
      },
      "outputs": [],
      "source": [
        "def predict(path_or_url, model, transforms_fn, categories, topk=1, headers=None):\n",
        "    if path_or_url.startswith('http'):\n",
        "        img = get_image_from_url(path_or_url, headers=headers)\n",
        "    else:\n",
        "        img = Image.open(path_or_url)\n",
        "\n",
        "    # Apply the transformation to the image\n",
        "    # write your code here\n",
        "    preproc_img = transforms_fn(img)\n",
        "\n",
        "    # If the transformation doesn't return a mini-batch\n",
        "    # We make one ourselves by unsqueezing the first dimension\n",
        "    if len(preproc_img.shape) == 3:\n",
        "        preproc_img = preproc_img.unsqueeze(0)\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    # write your code here\n",
        "    model.eval()\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Make predictions (logits)\n",
        "    pred = model(preproc_img)\n",
        "\n",
        "    # Compute probabilities out of the predicted logits\n",
        "    # and then get the topk values and indices\n",
        "    # write your code here\n",
        "    probabilities = torch.nn.functional.softmax(pred[0], dim=0)\n",
        "    values, indices = torch.topk(probabilities, topk)\n",
        "\n",
        "    return [{'label': categories[i], 'value': v.item()} for i, v in zip(indices, values)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11621c6c",
      "metadata": {
        "id": "11621c6c"
      },
      "source": [
        "Use the metadata from your model's weights as arguments to the function you wrote, and let's make a prediction using an image's URL:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "398bf19c",
      "metadata": {
        "id": "398bf19c"
      },
      "outputs": [],
      "source": [
        "# write your code here\n",
        "transforms_fn = weights.transforms()\n",
        "categories = weights.meta['categories']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e551a2a",
      "metadata": {
        "id": "2e551a2a"
      },
      "source": [
        "Let's make a prediction using an image's URL:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec6f5245",
      "metadata": {
        "id": "ec6f5245"
      },
      "outputs": [],
      "source": [
        "url = 'https://upload.wikimedia.org/wikipedia/commons/c/ce/Daisy_G%C3%A4nsebl%C3%BCmchen_Bellis_perennis_01.jpg'\n",
        "# Complying with Wikimedia User Agent's policy: https://meta.wikimedia.org/wiki/User-Agent_policy\n",
        "headers = {'User-Agent': 'CoolBot/0.0 (https://example.org/coolbot/; coolbot@example.org)'}\n",
        "\n",
        "# Call the predict function on an URL of an image, like the one above\n",
        "# Don't forget to pass the headers as argument\n",
        "# write your code here\n",
        "predict(url, model, transforms_fn, categories, headers=headers)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
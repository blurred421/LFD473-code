{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d8184ce",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/blurred421/LFD473-code/blob/main/labs/jb_Solved%20Lab%204.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-BsKDsa5LS9D",
   "metadata": {
    "id": "-BsKDsa5LS9D"
   },
   "source": [
    "# Lab Instructions\n",
    "\n",
    "In the lab, you're presented a task such as building a dataset, training a model, or writing a training loop, and we'll provide the code structured in such a way that you can fill in the blanks in the code using the knowledge you acquired in the chapters that precede the lab. You should be able to find appropriate snippets of code in the course content that work well in the lab with minor or no adjustments.\n",
    "\n",
    "The blanks in the code are indicated by ellipsis (`...`) and comments (`# write your code here`).\n",
    "\n",
    "In some cases, we'll provide you partial code to ensure the right variables are populated and any code that follows it runs accordingly.\n",
    "\n",
    "```python\n",
    "# write your code here\n",
    "x = ...\n",
    "```\n",
    "\n",
    "The solution should be a single statement that replaces the ellipsis, such as:\n",
    "\n",
    "```python\n",
    "# write your code here\n",
    "x = [0, 1, 2]\n",
    "```\n",
    "\n",
    "In some other cases, when there is no new variable being created, the blanks are shown like in the example below:\n",
    "\n",
    "```python\n",
    "# write your code here\n",
    "...\n",
    "```\n",
    "\n",
    "Although we're showing you only a single ellipsis (`...`), you may have to write more than one line of code to complete the step, such as:\n",
    "\n",
    "```python\n",
    "# write your code here\n",
    "for i, xi in enumerate(x):\n",
    "    x[i] = xi * 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GjQj9A4pLS9F",
   "metadata": {
    "id": "GjQj9A4pLS9F"
   },
   "source": [
    "## Installation Notes\n",
    "\n",
    "To run this notebook on Google Colab, you will need to install the following library: datasets.\n",
    "\n",
    "In Google Colab, you can run the following command to install this library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "i-nMEgglLS9G",
   "metadata": {
    "id": "i-nMEgglLS9G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting filelock (from datasets)\n",
      "  Downloading filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting numpy>=1.17 (from datasets)\n",
      "  Downloading numpy-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/student/LFT/myenv/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.13-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /home/student/LFT/myenv/lib/python3.11/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/student/LFT/myenv/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.5.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/student/LFT/myenv/lib/python3.11/site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/student/LFT/myenv/lib/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/student/LFT/myenv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/student/LFT/myenv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/student/LFT/myenv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/student/LFT/myenv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/student/LFT/myenv/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/student/LFT/myenv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading aiohttp-3.11.13-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "Downloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading numpy-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m02\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading filelock-3.17.0-py3-none-any.whl (16 kB)\n",
      "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohappyeyeballs-2.5.0-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (274 kB)\n",
      "Downloading multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "Downloading propcache-0.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (231 kB)\n",
      "Downloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (344 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, tqdm, pyarrow, propcache, numpy, multidict, fsspec, frozenlist, filelock, dill, aiohappyeyeballs, yarl, pandas, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohappyeyeballs-2.5.0 aiohttp-3.11.13 aiosignal-1.3.2 datasets-3.3.2 dill-0.3.8 filelock-3.17.0 frozenlist-1.5.0 fsspec-2024.12.0 huggingface-hub-0.29.3 multidict-6.1.0 multiprocess-0.70.16 numpy-2.2.3 pandas-2.2.3 propcache-0.3.0 pyarrow-19.0.1 pytz-2025.1 tqdm-4.67.1 tzdata-2025.1 xxhash-3.5.0 yarl-1.18.3\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17bded5",
   "metadata": {
    "id": "b17bded5"
   },
   "source": [
    "## 8.5 Lab 4: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1bb429",
   "metadata": {
    "id": "4d1bb429"
   },
   "source": [
    "In this lab, you'll fine-tune an encoder-based model to perform sentiment analysis on the Standford Sentiment Treebank (SST2) dataset. You'll load RoBERTa's sibling, XLM-RoBERTa, use its prescribed transformations to preprocess text in the SST2 dataset, and fine-tune (train) it for one epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585fd02c",
   "metadata": {
    "id": "585fd02c"
   },
   "source": [
    "### 8.5.1 Model\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step1.png)\n",
    "\n",
    "You'll use Hugging Face's `XLMRobertaForSequenceClassification` to perform binary classification (we have two classes, \"positive\" and \"negative\" sentiment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f670e1e9-5fab-42c4-9add-3a5573f57cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in /home/student/LFT/myenv/lib/python3.11/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /home/student/LFT/myenv/lib/python3.11/site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/student/LFT/myenv/lib/python3.11/site-packages (from transformers) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/student/LFT/myenv/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/student/LFT/myenv/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /home/student/LFT/myenv/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/student/LFT/myenv/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/student/LFT/myenv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/student/LFT/myenv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/student/LFT/myenv/lib/python3.11/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/student/LFT/myenv/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/student/LFT/myenv/lib/python3.11/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/student/LFT/myenv/lib/python3.11/site-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "Installing collected packages: safetensors, regex, tokenizers, transformers\n",
      "Successfully installed regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.0 transformers-4.49.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e4a1955-de24-46ad-9fde-c49049424158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in /home/student/LFT/myenv/lib/python3.11/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/student/LFT/myenv/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /home/student/LFT/myenv/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/student/LFT/myenv/lib/python3.11/site-packages (from torch) (2024.12.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch)\n",
      "  Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/student/LFT/myenv/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:04\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: triton, nvidia-cusparselt-cu12, mpmath, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "Successfully installed mpmath-1.3.0 networkx-3.4.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.6.0 triton-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6453fb87",
   "metadata": {
    "id": "6453fb87"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nXLMRobertaForSequenceClassification requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m XLMRobertaForSequenceClassification\n\u001b[32m      2\u001b[39m repo_id = \u001b[33m\"\u001b[39m\u001b[33mFacebookAI/xlm-roberta-base\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m model = \u001b[43mXLMRobertaForSequenceClassification\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m(repo_id, num_labels=\u001b[32m2\u001b[39m)\n\u001b[32m      5\u001b[39m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LFT/myenv/lib/python3.11/site-packages/transformers/utils/import_utils.py:1736\u001b[39m, in \u001b[36mDummyObject.__getattribute__\u001b[39m\u001b[34m(cls, key)\u001b[39m\n\u001b[32m   1734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key.startswith(\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key != \u001b[33m\"\u001b[39m\u001b[33m_from_config\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1735\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LFT/myenv/lib/python3.11/site-packages/transformers/utils/import_utils.py:1724\u001b[39m, in \u001b[36mrequires_backends\u001b[39m\u001b[34m(obj, backends)\u001b[39m\n\u001b[32m   1722\u001b[39m failed = [msg.format(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[32m   1723\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[32m-> \u001b[39m\u001b[32m1724\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(failed))\n",
      "\u001b[31mImportError\u001b[39m: \nXLMRobertaForSequenceClassification requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaForSequenceClassification\n",
    "repo_id = \"FacebookAI/xlm-roberta-base\"\n",
    "\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(repo_id, num_labels=2)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f10c74",
   "metadata": {
    "id": "52f10c74"
   },
   "source": [
    "### 8.5.2 Dataset\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step1.png)\n",
    "\n",
    "Now, you will load Hugging Face's [\"Stanford Sentiment Treebank (SST2)\"](https://huggingface.co/datasets/stanfordnlp/sst2) dataset. It is already split into `train`, `validation`, and `test` sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b37a60",
   "metadata": {
    "id": "65b37a60"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "datasets = load_dataset('stanfordnlp/sst2')\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f933d20a",
   "metadata": {
    "id": "f933d20a"
   },
   "source": [
    "Let's take a look at one data point from the SST2 dataset. Just run the code below as is to visualize the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d15fa20",
   "metadata": {
    "id": "3d15fa20"
   },
   "outputs": [],
   "source": [
    "row = datasets['train'][0]\n",
    "text, label = row['sentence'], row['label']\n",
    "text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377fe053",
   "metadata": {
    "id": "377fe053"
   },
   "source": [
    "Each data point is a dictionary, containing a line of text, and the corresponding label - the sentiment (0 for negative, 1 for positive)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eebde7e",
   "metadata": {
    "id": "6eebde7e"
   },
   "source": [
    "### 8.5.3 Tokenizer\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step3.png)\n",
    "\n",
    "You already know the drill: you must preprocess the input (the text) using the prescribed transformation for the model you're using, so it gets tokenized, converted into token ids, and prependend/appended with the appropriate special tokens.\n",
    "\n",
    "Load XLM-RoBERTa's tokenizer and write a function that takes a dictionary with the `sentence` key (it may have other keys as well) and returns a dictionary with `input_ids`, `attention_mask` keys (remember that the `map()` method of HF datasets work by _merging_ dictionaries):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c217192",
   "metadata": {
    "id": "0c217192"
   },
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer\n",
    "\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18d864c",
   "metadata": {
    "id": "f18d864c"
   },
   "outputs": [],
   "source": [
    "def apply_transform(row):\n",
    "    text = row['sentence']\n",
    "    # Use the transform_fn you retrieved in the previous cell to\n",
    "    # preprocess the text\n",
    "    # write your code here\n",
    "    return tokenizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd58738",
   "metadata": {
    "id": "dcd58738"
   },
   "source": [
    "Let's apply your function to our data point to see if it is working as expected (just run the code below as is to visualize the output):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca73f45",
   "metadata": {
    "id": "6ca73f45"
   },
   "outputs": [],
   "source": [
    "apply_transform(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TlQPes_WLS9Z",
   "metadata": {
    "id": "TlQPes_WLS9Z"
   },
   "source": [
    "Now, apply the function to every row in our datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6Q6yFG-oLS9a",
   "metadata": {
    "id": "6Q6yFG-oLS9a"
   },
   "outputs": [],
   "source": [
    "datasets = datasets.map(apply_transform)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kjHX6wEiLS9b",
   "metadata": {
    "id": "kjHX6wEiLS9b"
   },
   "source": [
    "To keep our datasets tidy, selct only the columns we're interested in (`input_ids` and `label`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ap93fqF2LS9c",
   "metadata": {
    "id": "Ap93fqF2LS9c"
   },
   "outputs": [],
   "source": [
    "datasets = datasets.select_columns(['input_ids', 'label'])\n",
    "datasets['train'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ec29d1",
   "metadata": {
    "id": "32ec29d1"
   },
   "source": [
    "Did you notice the transformation is returning a regular Python list of token ids, not a PyTorch tensor? Remember, we cannot make a tensor out of lists of different lengths (see section 6.3.3). The solution? Padding the shorter sentences, so they all have the same length.\n",
    "\n",
    "But, how can we think of padding sentences if we don't have a mini-batch yet? We delegate this job to the dataloaders's collate function!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d570206",
   "metadata": {
    "id": "0d570206"
   },
   "source": [
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/data_step5.png)\n",
    "\n",
    "So far, we've been using data loaders without specifying a collate function, that is, we're using its default collate function. For tabular data, the default collator is more than enough. It simply stacks several data points together and, since they all have the same size, it works smoothly. But this strategy breaks apart when we're dealing with sequences of different lengths, as we've already experienced while trying to make a tensor out of them.\n",
    "\n",
    "Just like before, padding is the solution for our problem, and we're using a [collator](https://huggingface.co/learn/nlp-course/en/chapter3/2#dynamic-padding) designed to automatically pad the sequences before stacking them together: HF's `DataCollatorWithPadding`. It takes the tokenizer as an argument in order to determine which token is the padding token, and which side (left or right) should be padded.\n",
    "\n",
    "Let's try it on a slice of four sequences from our training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2G60AxHGLS9f",
   "metadata": {
    "id": "2G60AxHGLS9f"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "data_collator(datasets['train'][:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yHFLtkaXLS9g",
   "metadata": {
    "id": "yHFLtkaXLS9g"
   },
   "source": [
    "You can easily recognize the padding tokens sitting at the right end of the sequences (a sequence of ones). Moreover, the location of every padding token is indicated by the sequence's corresponding attention mask. The masks tell the model which tokens should be considered (value of one) or ignored (value of zero).\n",
    "\n",
    "Next, let's assign this data collator to each dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c804f3",
   "metadata": {
    "id": "76c804f3"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloaders = {}\n",
    "# write your code here\n",
    "dataloaders['train'] = DataLoader(dataset=datasets['train'], batch_size=16, shuffle=True, collate_fn=data_collator)\n",
    "dataloaders['val'] = DataLoader(dataset=datasets['validation'], batch_size=16, shuffle=True, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923f3846",
   "metadata": {
    "id": "923f3846"
   },
   "source": [
    "Now, let's fetch a mini-batch from our data loader (just run the code below as is to visualize the output):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05babc84",
   "metadata": {
    "id": "05babc84"
   },
   "outputs": [],
   "source": [
    "dl_out = next(iter(dataloaders['train']))\n",
    "dl_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eb7806",
   "metadata": {
    "id": "d4eb7806"
   },
   "source": [
    "As you can see, there are plenty of padding tokens there. The collator will always pad the sequences to match the longest sequence in a particular mini-batch. This means that mini-batches may have sequences of different lengths (when compared across mini-batches but not inside the same one)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171714e6",
   "metadata": {
    "id": "171714e6"
   },
   "source": [
    "### 8.5.4 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260d405e",
   "metadata": {
    "id": "260d405e"
   },
   "source": [
    "Now, it is time to write a training loop to fine-tune your XLM-RoBERTa model on the SST2 dataset. This is a large model, and the training set has over 60,000 data points, so you can train it over a single epoch, that is, looping over the mini-batches from the datapipe (or data loader) only once. For the sake of speed, keep the evalution for the end only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9A98-FXiLS9s",
   "metadata": {
    "id": "9A98-FXiLS9s"
   },
   "source": [
    "#### 8.5.4.1 Loss Function\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step2.png)\n",
    "\n",
    "Sentiment analysis is a classification task, so we need to use the appropriate loss function for the task. Even though it is a binary classification, RoBERTa's classification head is actually producing two logits instead of one, so you have to use `CrossEntropyLoss` (which can handle two or more logits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330c949d",
   "metadata": {
    "id": "330c949d"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ylpU39c4LS-E",
   "metadata": {
    "id": "ylpU39c4LS-E"
   },
   "source": [
    "This step is actually redundant now. Since we're using a HF model, the loss is automatically returned when the model is in training model. We simply retrieve the loss from the output's `loss` attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LiMz78USLS-F",
   "metadata": {
    "id": "LiMz78USLS-F"
   },
   "source": [
    "#### 8.5.4.2 Optimizer\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UfDSj2Z0LS-F",
   "metadata": {
    "id": "UfDSj2Z0LS-F"
   },
   "source": [
    "Although `Adam` is the optimizer of choice, we suggest you try out `AdamW`, a modified version that is also commonly used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FXHNG8wvLS-G",
   "metadata": {
    "id": "FXHNG8wvLS-G"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# suggested learning rate\n",
    "lr = 1e-5\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fvixsnlYLS-H",
   "metadata": {
    "id": "fvixsnlYLS-H"
   },
   "source": [
    "#### 8.4.4.2 Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YveaXzXhLS-I",
   "metadata": {
    "id": "YveaXzXhLS-I"
   },
   "source": [
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step4.png)\n",
    "\n",
    "So far, we haven't logged or inspected our losses in real-time. Why bother, if it takes only a minute to train the model? This time is different, though: fine-tuning RoBERTa on more than 67,000 data points, even for a single epoch, will take about 15 min or so in Google Colab. So, let's use TensorBoard to see how our loss is doing as training progresses.\n",
    "\n",
    "First, we need to load it using the corresponding Jupyter magic (just run the code below as is to load TensorBoard):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vWc73L7FLS-V",
   "metadata": {
    "id": "vWc73L7FLS-V"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OVEgrpL0LS-W",
   "metadata": {
    "id": "OVEgrpL0LS-W"
   },
   "source": [
    "Next, we need to create an instance of the `SummaryWriter` to be able to send loss values to TensorBoard. Just run the code below as is to create it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8FBbWMnLLS-X",
   "metadata": {
    "id": "8FBbWMnLLS-X"
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('runs/roberta')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pJ1Sem-1LS-X",
   "metadata": {
    "id": "pJ1Sem-1LS-X"
   },
   "source": [
    "Now, it's your turn to write the missing parts of the training loop below. We have already taken care of the sending the losses to TensorBoard for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b75295",
   "metadata": {
    "id": "a7b75295"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "batch_losses = []\n",
    "\n",
    "## Training\n",
    "for i, batch in tqdm(enumerate(dataloaders['train'])):\n",
    "    batch_features = batch['input_ids']\n",
    "    batch_targets = batch['labels']\n",
    "    batch_masks = batch['attention_mask']\n",
    "    # Set the model's mode\n",
    "    # write your code here\n",
    "    model.train()\n",
    "\n",
    "    # Send input_ids, labels, and attention masks to the device\n",
    "    # write your code here\n",
    "    batch_features = batch_features.to(device)\n",
    "    batch_targets = batch_targets.to(device)\n",
    "    batch_masks = batch_masks.to(device)\n",
    "\n",
    "    # Step 1 - forward pass\n",
    "    # write your code here\n",
    "    output = model(input_ids=batch_features,\n",
    "                        attention_mask=batch_masks,\n",
    "                        labels=batch_targets)\n",
    "    prediction = output.logits\n",
    "\n",
    "    # Step 2 - computing the loss\n",
    "    # write your code here\n",
    "    loss = output.loss\n",
    "\n",
    "    # Step 3 - computing the gradients\n",
    "    # Tip: it requires a single method call to backpropagate gradients\n",
    "    # write your code here\n",
    "    loss.backward()\n",
    "\n",
    "    batch_losses.append(loss.item())\n",
    "\n",
    "    writer.add_scalars(main_tag='loss',\n",
    "                       tag_scalar_dict={'training': loss.item()},\n",
    "                       global_step=i)\n",
    "\n",
    "    # Step 4 - updating parameters and zeroing gradients\n",
    "    # Tip: it takes two calls to optimizer's methods\n",
    "    # write your code here\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "writer.close()\n",
    "\n",
    "## Validation\n",
    "with torch.inference_mode():\n",
    "    val_losses = []\n",
    "\n",
    "    #for i, (val_features, val_targets) in enumerate(dataloaders['val']):\n",
    "    for i, val in enumerate(dataloaders['val']):\n",
    "        val_features = val['input_ids']\n",
    "        val_targets = val['labels']\n",
    "        val_masks = val['attention_mask']\n",
    "        # Set the model's mode\n",
    "        # write your code here\n",
    "        model.eval()\n",
    "\n",
    "        # Send input_ids, labels, and attention masks to the device\n",
    "        # write your code here\n",
    "        val_features = val_features.to(device)\n",
    "        val_targets = val_targets.to(device)\n",
    "        val_masks = val_masks.to(device)\n",
    "\n",
    "        # Step 1 - forward pass\n",
    "        # write your code here\n",
    "        output = model(input_ids=val_features, attention_mask=val_masks, labels=val_targets)\n",
    "        predictions = output.logits\n",
    "\n",
    "        # Step 2 - computing the loss\n",
    "        # write your code here\n",
    "        loss = output.loss\n",
    "\n",
    "        val_losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uVlQkNwwLS-Y",
   "metadata": {
    "id": "uVlQkNwwLS-Y"
   },
   "source": [
    "By the end of it, your losses on TensorBoard should look more or less like this (if you drag the slider on the right to the maximum level of smoothing):\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch6/tensorboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724660c3",
   "metadata": {
    "id": "724660c3"
   },
   "source": [
    "### 8.5.5 Inference\n",
    "\n",
    "![](https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch0/model_step5.png)\n",
    "\n",
    "Write a function that takes some text (a sequence of words), a model, its tokenizer, and a list of target categories for the classification, and returns the most likely category and the corresponding probability.\n",
    "\n",
    "Since you're handling a single sequence, there's no need for any padding, but you still need to provide a tensor containing a mini-batch (of one) as input to the model.\n",
    "\n",
    "The model returns two logits, one for each class, so you must use the softmax function to convert them into probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a8d274",
   "metadata": {
    "id": "a9a8d274"
   },
   "outputs": [],
   "source": [
    "def predict(sequence, model, tokenizer, categories):\n",
    "    # Build a tensor of token ids out of the input sequence\n",
    "    # write your code here\n",
    "    token_ids = tokenizer(sequence, return_tensors='pt')['input_ids']\n",
    "\n",
    "    # Set the model to the appropriate mode\n",
    "    # write your code here\n",
    "    model.eval()\n",
    "\n",
    "    device = next(iter(model.parameters())).device\n",
    "\n",
    "    # Use the model to make predictions/logits\n",
    "    # Tip: Don't forget to send the input to the same device as the model\n",
    "    # write your code here\n",
    "    pred = model(token_ids.to(device)).logits\n",
    "\n",
    "    # Compute the probabilities corresponding to the logits\n",
    "    # and return the top value and index\n",
    "    # write your code here\n",
    "    probabilities = torch.nn.functional.softmax(pred[0], dim=0)\n",
    "    values, indices = torch.topk(probabilities, 1)\n",
    "\n",
    "    return [{'label': categories[i], 'value': v.item()} for i, v in zip(indices, values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b587f62f",
   "metadata": {
    "id": "b587f62f"
   },
   "source": [
    "Now, try out your prediction function and fine-tuned model (just run the code cells below as they are to visualize their outputs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe87a17e",
   "metadata": {
    "id": "fe87a17e"
   },
   "outputs": [],
   "source": [
    "categories = ['negative', 'positive']\n",
    "text = \"I am really liking this course\"\n",
    "predict(text, model, tokenizer, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b50f0c7",
   "metadata": {
    "id": "7b50f0c7"
   },
   "outputs": [],
   "source": [
    "text = \"This course is too complicated!\"\n",
    "predict(text, model, tokenizer, categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578f344d",
   "metadata": {
    "id": "578f344d"
   },
   "source": [
    "That's cool, but what if we could perform sentiment analysis out-of-the-box? That's what we'll do in the second part of Chapter 6."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

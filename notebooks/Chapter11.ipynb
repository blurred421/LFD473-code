{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blurred421/LFD473-code/blob/main/notebooks/Chapter11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "706bdbf0"
      },
      "source": [
        "# Chapter 11: Serving Models with TorchServe"
      ],
      "id": "706bdbf0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2906de38"
      },
      "outputs": [],
      "source": [
        "!pip install torch-model-archiver torchserve captum pyngrok"
      ],
      "id": "2906de38"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "860cbba6"
      },
      "source": [
        "## 11.2 Learning Objectives\n",
        "\n",
        "By the end of this chapter, you should be able to:\n",
        "- understand, build, and assemble the necessary components into a model archive\n",
        "- serve a trained model locally using TorchServe"
      ],
      "id": "860cbba6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dcdd57d"
      },
      "source": [
        "## 11.3 Archiving and Serving Models"
      ],
      "id": "7dcdd57d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iN52_d0ebee"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/dvgodoy/assets/releases/download/model/fomo_model.pth"
      ],
      "id": "9iN52_d0ebee"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlYk-bqXfIBB",
        "outputId": "1161e176-91d4-4bb5-8d10-27d3dfa13844"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "/root/.cache/torch/hub/pytorch_vision_main/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "repo = 'pytorch/vision:v0.15.2'\n",
        "model = torch.hub.load(repo, 'resnet18', weights=None)\n",
        "model.fc = nn.Linear(512, 4)\n",
        "\n",
        "state = torch.load('fomo_model.pth', map_location='cpu')\n",
        "model.load_state_dict(state)"
      ],
      "id": "YlYk-bqXfIBB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f58045c"
      },
      "source": [
        "### 11.3.1 Model Archiver\n",
        "\n",
        "Let's start with the model archive (`.mar`) file, a collection of files and folders zipped together that contains:\n",
        "- a `MAR-INF` folder with a `MANIFEST.json` file inside that describes the contents of the model archive itself, such as model and archiver versions, and the files that make up the archive\n",
        "- a serialized file containing the model's weights/state (`--serialized-file` argument)\n",
        "- a Python file containing only one class definition of our model's class inherited from `nn.Module` (only required if the model isn't scripted - more on that later) (`--model-file` argument)\n",
        "- an optional Python file containing one class definition of the handler's class inherited from `ts.torch_handler.BaseHandler` that performs the necessary transformations for pre- and post-processing  OR the name of a predefined handler (`--handler` argument)\n",
        "- an optional extra file `index_to_name.json` for mapping predicted class indices to its corresponding category names (automatically used by some predefined handlers) (`--extra-files` argument)\n",
        "\n",
        "It is typical to assemble the model archive file through the command line interface:\n",
        "\n",
        "```\n",
        "torch-model-archiver --model-name <your_model_name> \\\n",
        "                     --version <your_model_version> \\\n",
        "                     --model-file <your_model_file>.py \\\n",
        "                     --serialized-file <your_model_name>.pth \\\n",
        "                     --handler <handler-script OR name> \\\n",
        "                     --extra-files ./index_to_name.json\n",
        "```\n",
        "\n",
        "However, let's take a closer look at each one of its components and assemble it ourselves instead."
      ],
      "id": "7f58045c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dfd4a88"
      },
      "source": [
        "### 11.3.2 Model File\n",
        "\n",
        "We need to:\n",
        "- define our own class\n",
        "- create an instance of an untrained ResNet18 model\n",
        "- replace its head (`fc` layer) with our own\n",
        "- update our own class internal dictionary with the entries from ResNet's dictionary\n",
        "- set ResNet's forward pass to our own class using `setattr`\n",
        "\n",
        "It looks like this:"
      ],
      "id": "1dfd4a88"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbe811f3"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import resnet18\n",
        "\n",
        "class FOMONet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create an instance of an untrained ResNet18\n",
        "        resnet = resnet18(weights=None)\n",
        "        # Modifies the architecture to our task\n",
        "        resnet.fc = nn.Linear(512, 4)\n",
        "\n",
        "        # Replicate ResNet's modified architecture to FOMONet\n",
        "        self.__dict__.update(resnet.__dict__)\n",
        "        # Replicate Resnet's forward method to FOMONet\n",
        "        setattr(self, 'forward', resnet.forward)"
      ],
      "id": "cbe811f3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b17dee7e",
        "outputId": "1f4b71fb-2a6e-4e64-9cd4-a3b188df6d37"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fomo = FOMONet()\n",
        "fomo.load_state_dict(model.state_dict())"
      ],
      "id": "b17dee7e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eed5b52c",
        "outputId": "93210edf-1535-4627-91dd-c26ff2a1b6a4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[ 0.2412, -2.8556, -1.1869,  0.8597]], grad_fn=<AddmmBackward0>),\n",
              " tensor([[ 0.2412, -2.8556, -1.1869,  0.8597]], grad_fn=<AddmmBackward0>))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fomo.eval()\n",
        "model.eval()\n",
        "\n",
        "torch.manual_seed(32)\n",
        "x = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "fomo(x), model.cpu()(x)"
      ],
      "id": "eed5b52c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75308290"
      },
      "outputs": [],
      "source": [
        "model_file_script = \"\"\"\n",
        "import torch.nn as nn\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "class FOMONet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create an instance of an untrained ResNet18\n",
        "        resnet = resnet18(weights=None)\n",
        "        # Modifies the architecture to our task\n",
        "        resnet.fc = nn.Linear(512, 4)\n",
        "\n",
        "        # Replicate ResNet's modified architecture to FOMONet\n",
        "        self.__dict__.update(resnet.__dict__)\n",
        "        # Replicate Resnet's forward method to FOMONet\n",
        "        setattr(self, 'forward', resnet.forward)\n",
        "\"\"\"\n",
        "\n",
        "with open('model_file.py', 'w') as fp:\n",
        "    fp.write(model_file_script)"
      ],
      "id": "75308290"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4042419b"
      },
      "source": [
        "### 11.3.3 Scripted Models\n",
        "\n",
        "\"*TorchScript is a way to create serializable and optimizable models from PyTorch code. Any TorchScript program can be saved from a Python process and loaded in a process where there is no Python dependency.*\"\n",
        "\n",
        "Source: [Torchscript](https://pytorch.org/docs/stable/jit.html)\n",
        "\n",
        "The key element here is \"*no Python dependency*\", meaning the model can be run in a standalone C++ program, for example. This preserves the best of both worlds: the ease and friendliness of the Python language for development, and the speed and reliability of the C++ language for deploying in production."
      ],
      "id": "4042419b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e7631af"
      },
      "outputs": [],
      "source": [
        "# once it is scripted, there is no need for the model class def anymore\n",
        "scripted_model = torch.jit.script(model)"
      ],
      "id": "3e7631af"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af97c6fa"
      },
      "source": [
        "### 11.3.4 Serialized File"
      ],
      "id": "af97c6fa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5151de67"
      },
      "outputs": [],
      "source": [
        "# We already saved the model to disk in the previous chapter\n",
        "# eager mode version\n",
        "torch.save(model.state_dict(), 'fomo_model.pth')\n",
        "\n",
        "# scripted version\n",
        "scripted_model.save(\"fomo_model.pt\")"
      ],
      "id": "5151de67"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfb9e487"
      },
      "source": [
        "### 11.3.5 Inference Handler\n",
        "\n",
        "There are several implemented [default handlers](https://pytorch.org/serve/default_handlers.html) in Torchserve:\n",
        "- `image_classifier`\n",
        "- `object_detector`\n",
        "- `text_classifier`\n",
        "- `image_segmenter`\n",
        "\n",
        "The first three handles also implement mapping the predicted class to its corresponding names/categories using an standard `index_to_name.json` extra file."
      ],
      "id": "bfb9e487"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3848245e"
      },
      "source": [
        "#### 11.3.5.1 Initialize\n",
        "\n",
        "```python\n",
        "def initialize(self, context):\n",
        "    \"\"\"Initialize function loads the model.pt file and initialized the model object.\n",
        "       First try to load torchscript else load eager mode state_dict based model.\n",
        "    \"\"\"\n",
        "    model_file = self.manifest[\"model\"].get(\"modelFile\", \"\")\n",
        "    if model_file:\n",
        "        self.model = self._load_pickled_model(model_dir, model_file, self.model_pt_path)\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "    elif self.model_pt_path.endswith(\".pt\"):\n",
        "        self.model = self._load_torchscript_model(self.model_pt_path)\n",
        "        self.model.eval()\n",
        "```"
      ],
      "id": "3848245e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14be43e5"
      },
      "source": [
        "#### 11.3.5.2 Handle\n",
        "\n",
        "```python\n",
        "def handle(self, data, context):\n",
        "    \"\"\"Entry point for default handler. It takes the data from the input request and returns\n",
        "       the predicted outcome for the input.\n",
        "    \"\"\"\n",
        "    data_preprocess = self.preprocess(data)\n",
        "    output = self.inference(data_preprocess)\n",
        "    output = self.postprocess(output)\n",
        "\n",
        "    return output\n",
        "```"
      ],
      "id": "14be43e5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c378c5d"
      },
      "source": [
        "#### 11.3.5.3 Preprocess\n",
        "\n",
        "```python\n",
        "def preprocess(self, data):\n",
        "    \"\"\"\n",
        "    Preprocess function to convert the request input to a tensor(Torchserve supported format).\n",
        "    The user needs to override to customize the pre-processing\n",
        "    \"\"\"\n",
        "    images = []\n",
        "\n",
        "    for row in data:\n",
        "        # Compat layer: normally the envelope should just return the data\n",
        "        # directly, but older versions of Torchserve didn't have envelope.\n",
        "        image = row.get(\"data\") or row.get(\"body\")\n",
        "        if isinstance(image, str):\n",
        "            # if the image is a string of bytesarray.\n",
        "            image = base64.b64decode(image)\n",
        "\n",
        "        # If the image is sent as bytesarray\n",
        "        if isinstance(image, (bytearray, bytes)):\n",
        "            image = Image.open(io.BytesIO(image))\n",
        "            image = self.image_processing(image)\n",
        "        else:\n",
        "            # if the image is a list\n",
        "            image = torch.FloatTensor(image)\n",
        "\n",
        "        images.append(image)\n",
        "\n",
        "    return torch.stack(images).to(self.device)\n",
        "```"
      ],
      "id": "4c378c5d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a0cdd07"
      },
      "source": [
        "Let's take a quick look at the `image_processing()` function that's called by the `preprocess()` method:"
      ],
      "id": "3a0cdd07"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e67e050",
        "outputId": "36bae002-1d49-45f3-fd86-4de6a475f08d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:ts.torch_handler.base_handler:proceeding without onnxruntime\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Compose(\n",
              "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=warn)\n",
              "    CenterCrop(size=(224, 224))\n",
              "    ToTensor()\n",
              "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              ")"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ts.torch_handler.image_classifier import ImageClassifier\n",
        "\n",
        "ImageClassifier.image_processing"
      ],
      "id": "5e67e050"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a427b9e",
        "outputId": "ef5f5540-769a-4679-a0e9-9177f5884f26"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ImageClassification(\n",
              "    crop_size=[224]\n",
              "    resize_size=[256]\n",
              "    mean=[0.485, 0.456, 0.406]\n",
              "    std=[0.229, 0.224, 0.225]\n",
              "    interpolation=InterpolationMode.BILINEAR\n",
              ")"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchvision.models import get_weight\n",
        "\n",
        "weights = get_weight('ResNet18_Weights.DEFAULT')\n",
        "weights.transforms()"
      ],
      "id": "6a427b9e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmRWcNYLyttW"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch9/fig_0_100.jpg"
      ],
      "id": "MmRWcNYLyttW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8f01145",
        "outputId": "439793f6-4d27-4c50-dddf-665cddcd6d56"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(True)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from PIL import Image\n",
        "\n",
        "img = Image.open('./fig_0_100.jpg')\n",
        "\n",
        "(ImageClassifier.image_processing(img) == weights.transforms()(img)).all()"
      ],
      "id": "d8f01145"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f2de09c"
      },
      "source": [
        "#### 11.3.5.4 Inference\n",
        "\n",
        "```python\n",
        "def inference(self, data, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    The Inference Function is used to make a prediction call on the given input request.\n",
        "    The user needs to override the inference function to customize it.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        marshalled_data = data.to(self.device)\n",
        "        results = self.model(marshalled_data, *args, **kwargs)\n",
        "    return results\n",
        "```"
      ],
      "id": "1f2de09c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86477c8b"
      },
      "source": [
        "#### 11.3.5.5 Postprocess\n",
        "\n",
        "```python\n",
        "def postprocess(self, data):\n",
        "    \"\"\"\n",
        "    The post process function makes use of the output from the inference and converts into a\n",
        "    Torchserve supported response output.\n",
        "    \"\"\"\n",
        "    ps = F.softmax(data, dim=1)\n",
        "    probs, classes = torch.topk(ps, self.topk, dim=1)\n",
        "    probs = probs.tolist()\n",
        "    classes = classes.tolist()\n",
        "    return map_class_to_label(probs, self.mapping, classes)\n",
        "```"
      ],
      "id": "86477c8b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0ee61f2"
      },
      "source": [
        "#### 11.3.5.6 Custom Handler"
      ],
      "id": "f0ee61f2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "815d7cf5"
      },
      "outputs": [],
      "source": [
        "handler_file_script = \"\"\"\n",
        "from ts.torch_handler.image_classifier import ImageClassifier\n",
        "\n",
        "class FOMOHandler(ImageClassifier):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "      # By default, ImageClassifier uses top-5 classes\n",
        "      # but our task has only 4, so we need to tweak it\n",
        "      self.set_max_result_classes(4)\n",
        "\"\"\"\n",
        "\n",
        "with open('handler_file.py', 'w') as fp:\n",
        "    fp.write(handler_file_script)"
      ],
      "id": "815d7cf5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd336550"
      },
      "source": [
        "### 11.3.6 Extra Files"
      ],
      "id": "fd336550"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e3cf975"
      },
      "outputs": [],
      "source": [
        "# We didn't load the dataset in this chapter, so we're building the dict manually\n",
        "# class_to_idx = datasets['train'].class_to_idx\n",
        "\n",
        "class_to_idx = {'Fig': 0, 'Mandarine': 1, 'Onion White': 2, 'Orange': 3}"
      ],
      "id": "6e3cf975"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86f0dab9",
        "outputId": "dcb764ba-d716-4db0-a0d8-c8d0c0dc16ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: 'Fig', 1: 'Mandarine', 2: 'Onion White', 3: 'Orange'}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "index_to_name = {v: k for k, v in class_to_idx.items()}\n",
        "index_to_name"
      ],
      "id": "86f0dab9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3883ada"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('index_to_name.json', 'w') as f:\n",
        "    json.dump(index_to_name, f)"
      ],
      "id": "b3883ada"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab4ab790"
      },
      "source": [
        "### 11.3.7 Packaging\n",
        "\n",
        "```\n",
        "torch-model-archiver --model-name FOMO> \\\n",
        "                     --version 1.0 \\\n",
        "                     --model-file ./model_file.py \\\n",
        "                     --serialized-file fomo_model.pth \\\n",
        "                     --handler ./handler_file.py \\\n",
        "                     --extra-files ./index_to_name.json\n",
        "```"
      ],
      "id": "ab4ab790"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5_jJT25FUzk"
      },
      "outputs": [],
      "source": [
        "!mkdir ./model_store"
      ],
      "id": "Y5_jJT25FUzk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "044917c6"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from model_archiver.model_packaging import generate_model_archive\n",
        "\n",
        "sys.argv = ['',\n",
        "            '--model-name', 'FOMO',\n",
        "            '--version', '1.0',\n",
        "            '--model-file', 'model_file.py',\n",
        "            '--serialized-file', 'fomo_model.pth',\n",
        "            '--handler', 'handler_file.py',\n",
        "            '--extra-files', 'index_to_name.json',\n",
        "            '--export-path', './model_store',\n",
        "            '--force']\n",
        "\n",
        "generate_model_archive()"
      ],
      "id": "044917c6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9beb18f5"
      },
      "source": [
        "## 11.4 TorchServe\n",
        "\n",
        "[TorchServe](https://pytorch.org/serve/) is a flexible and easy to use tool for serving and scaling PyTorch eager mode and scripted models in production. It offers APIs for querying, managing, and analyzing the performance of its served models (by default, they are only accessible from localhost):\n",
        "\n",
        "- [Inference API](https://github.com/pytorch/serve/blob/master/docs/inference_api.md): it listens to port 8080, and it offers the following services\n",
        "  - description (`OPTIONS /`)\n",
        "  - health check (`GET /ping`)\n",
        "  - predictions (`POST {/predictions/{model_name}`)\n",
        "  - explanations (`POST /explanations/{model_name}`)\n",
        "  - kserve (`/v1/models/{model_name}:predict:`)\n",
        "  - kserve explanations (`/v1/models/{model_name}:explain:`)\n",
        "  \n",
        "- [Management API](https://github.com/pytorch/serve/blob/master/docs/management_api.md): it listens to port 8081, and it offers the following services\n",
        "  - description (`OPTIONS /`)\n",
        "  - list models (`GET /models`)\n",
        "  - describe a model (`GET /models/{model_name}`)\n",
        "  - register a model (`POST /models`)\n",
        "  - scale workers (`POST /models/{model_name}`)\n",
        "  - set default version (`PUT /models/{model_name}/{version}/set-default`)\n",
        "  - unregister a model (`DELETE /models/{model_name}/{version}`)\n",
        "  \n",
        "- [Metrics API](https://github.com/pytorch/serve/blob/master/docs/metrics_api.md): it listens to port 8082, and it returns Prometheus-formatted frontend and backend metrics, such as number of requests, CPU and memory utilization, handler and prediction time, and many more.\n",
        "\n",
        "```\n",
        "torchserve --start \\\n",
        "           --disable-token-auth \\\n",
        "           --model-store ./model_store \\\n",
        "           --models fomo=FOMO.mar \\\n",
        "           --ts-config config.properties\n",
        "```"
      ],
      "id": "9beb18f5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06345f96"
      },
      "outputs": [],
      "source": [
        "config_properties = \"\"\"\n",
        "inference_address=http://127.0.0.1:7777\n",
        "\"\"\"\n",
        "\n",
        "with open('config.properties', 'w') as fp:\n",
        "    fp.write(config_properties)"
      ],
      "id": "06345f96"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7f6429f"
      },
      "outputs": [],
      "source": [
        "from ts.model_server import start\n",
        "\n",
        "sys.argv = ['',\n",
        "            '--start',\n",
        "            '--disable-token-auth',\n",
        "            '--model-store', './model_store',\n",
        "            '--models', 'fomo=FOMO.mar',\n",
        "            '--ts-config', 'config.properties']\n",
        "start()"
      ],
      "id": "b7f6429f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e458c243",
        "outputId": "3ce26f4f-454d-4e27-90de-d1352d9dcaa7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Fig': 0.9934685230255127,\n",
              " 'Orange': 0.004324017558246851,\n",
              " 'Onion White': 0.0012627042597159743,\n",
              " 'Mandarine': 0.0009447108022868633}"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "with open('./fig_0_100.jpg', 'rb') as f:\n",
        "    data = f.read()\n",
        "\n",
        "response = requests.put('http://127.0.0.1:7777/predictions/fomo', data=data)\n",
        "response.json()"
      ],
      "id": "e458c243"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f64a41a",
        "outputId": "be7595a3-45c3-4572-d22e-961eec3d9a1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TorchServe has stopped.\n"
          ]
        }
      ],
      "source": [
        "#!torchserve --stop\n",
        "sys.argv = ['', '--stop']\n",
        "start()"
      ],
      "id": "8f64a41a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cb4111d"
      },
      "source": [
        "### 11.4.1 Ngrok (optional)\n",
        "\n",
        "\"*Online in One Line*\" reads the [ngrok](https://ngrok.com/) website. It is an easy and convenient way of serving your model through a tunnel, thus allowing it to handle incoming requests from the outside world in your own Jupyter Notebook.\n",
        "\n",
        "***\n",
        "**DISCLAIMER**: You should NOT use Google Colab notebooks as backend for your deployed models. This is just a proof-of-concept, and a way to make your model available to the world for a brief amount of time, so you can showcase it to your family, friends, or colleagues.\n",
        "***\n",
        "\n",
        "If you want to try the code below, you'll need to [signup](https://dashboard.ngrok.com/signup) for a free account on [ngrok](https://ngrok.com/) and, once you're done, you can install the [pyngrok](https://pypi.org/project/pyngrok/) package that takes care of downloading and installing ngrok:"
      ],
      "id": "4cb4111d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6811ea0"
      },
      "source": [
        "You'll need to copy your [authorization token](https://dashboard.ngrok.com/get-started/your-authtoken) and paste it in the appropriate command below:\n",
        "\n",
        "***\n",
        "**DISCLAIMER**: The responsibility for keeping your credentials and/or authorization tokens safe and private is your own. Make sure to remove any credentials and/or authorizations tokens from your notebook before saving or pushing it to public repositories, such as GitHub.\n",
        "***"
      ],
      "id": "b6811ea0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f621f45",
        "outputId": "a950a5e0-1a74-4283-914d-c13c01a06f8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "# Option 1\n",
        "# You can call ngrok with your token\n",
        "# Uncomment the line below and replace ... with your token\n",
        "# !ngrok authtoken ...\n",
        "\n",
        "# Option 2\n",
        "# Or you can save it to a configuration file\n",
        "# Uncomment the line below and replace ... with your token\n",
        "# !echo \"authtoken: ...\" >> /root/.ngrok2/ngrok.yml"
      ],
      "id": "5f621f45"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a78bdddb"
      },
      "source": [
        "Once ngrok is setup, let's start Torchserve once again with a few modifications in the `config.properties` file:\n",
        "\n",
        "***\n",
        "**DISCLAIMER**: CORS stands for cross-origin resource sharing, and the configuration below makes Torchserve wide open to requests from anywhere. You SHOULD NOT use these configuration parameters in production as they're not safe. The responsibility for ensuring the security of your application, model, and data, is your own.\n",
        "***"
      ],
      "id": "a78bdddb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef8fa355"
      },
      "outputs": [],
      "source": [
        "config_properties = \"\"\"\n",
        "inference_address=http://127.0.0.1:7777\n",
        "cors_allowed_origin=*\n",
        "cors_allowed_methods=GET, POST, PUT, OPTIONS\n",
        "\"\"\"\n",
        "\n",
        "with open('config_cors.properties', 'w') as fp:\n",
        "    fp.write(config_properties)"
      ],
      "id": "ef8fa355"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f57fa8c"
      },
      "outputs": [],
      "source": [
        "sys.argv = ['',\n",
        "            '--start',\n",
        "            '--model-store', './model_store',\n",
        "            '--models', 'fomo=FOMO.mar',\n",
        "            '--ts-config', 'config_cors.properties']\n",
        "start()"
      ],
      "id": "2f57fa8c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bc063d6",
        "outputId": "f43f3562-5934-4c89-b2e7-ae660d4fb919"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-07-02T12:14:34+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# <NgrokTunnel: \"http://<public_sub>.ngrok.io\" -> \"http://localhost:7777\">\n",
        "http_tunnel = ngrok.connect(7777, \"http\")"
      ],
      "id": "4bc063d6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "e3014e2b",
        "outputId": "bfcb68d1-b71f-495f-a97d-3ce8385e24fc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://f295-35-202-252-169.ngrok-free.app'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "http_tunnel.public_url"
      ],
      "id": "e3014e2b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5118d4aa",
        "outputId": "9bd3fb2e-eeab-408d-8893-831474567311"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Fig': 0.9934685230255127,\n",
              " 'Orange': 0.004324017558246851,\n",
              " 'Onion White': 0.0012627042597159743,\n",
              " 'Mandarine': 0.0009447108022868633}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open('./fig_0_100.jpg', 'rb') as f:\n",
        "    data = f.read()\n",
        "\n",
        "response = requests.put(f'{http_tunnel.public_url}/predictions/fomo', data=data)\n",
        "response.json()"
      ],
      "id": "5118d4aa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9fad67d"
      },
      "outputs": [],
      "source": [
        "ngrok.disconnect(http_tunnel.public_url)"
      ],
      "id": "a9fad67d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dd0aaf5",
        "outputId": "555412ca-3dbc-45a9-8bd7-1ed8be392851"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TorchServe has stopped.\n"
          ]
        }
      ],
      "source": [
        "sys.argv = ['', '--stop']\n",
        "start()"
      ],
      "id": "3dd0aaf5"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
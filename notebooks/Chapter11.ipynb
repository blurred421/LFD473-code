{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blurred421/LFD473-code/blob/main/notebooks/Chapter11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "706bdbf0"
      },
      "source": [
        "# Chapter 11: Serving Models with TorchServe"
      ],
      "id": "706bdbf0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2906de38"
      },
      "outputs": [],
      "source": [
        "!pip install torch-model-archiver torchserve captum pyngrok"
      ],
      "id": "2906de38"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "860cbba6"
      },
      "source": [
        "## 11.2 Learning Objectives\n",
        "\n",
        "By the end of this chapter, you should be able to:\n",
        "- understand, build, and assemble the necessary components into a model archive\n",
        "- serve a trained model locally using TorchServe"
      ],
      "id": "860cbba6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dcdd57d"
      },
      "source": [
        "## 11.3 Archiving and Serving Models"
      ],
      "id": "7dcdd57d"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9iN52_d0ebee",
        "outputId": "c52d3a51-6540-44ed-c670-e0e112c07415",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-12 15:11:16--  https://github.com/dvgodoy/assets/releases/download/model/fomo_model.pth\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/662216076/ef1f7d06-df52-4aee-b442-caf89a66872c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250312%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250312T151116Z&X-Amz-Expires=300&X-Amz-Signature=202b4a365753c4aa54fa4bb24fb645acd6d578fa40843028c363e8af816e279d&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dfomo_model.pth&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-03-12 15:11:16--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/662216076/ef1f7d06-df52-4aee-b442-caf89a66872c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250312%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250312T151116Z&X-Amz-Expires=300&X-Amz-Signature=202b4a365753c4aa54fa4bb24fb645acd6d578fa40843028c363e8af816e279d&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dfomo_model.pth&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 44793025 (43M) [application/octet-stream]\n",
            "Saving to: ‘fomo_model.pth’\n",
            "\n",
            "fomo_model.pth      100%[===================>]  42.72M   127MB/s    in 0.3s    \n",
            "\n",
            "2025-03-12 15:11:16 (127 MB/s) - ‘fomo_model.pth’ saved [44793025/44793025]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/dvgodoy/assets/releases/download/model/fomo_model.pth"
      ],
      "id": "9iN52_d0ebee"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlYk-bqXfIBB",
        "outputId": "f0b3bcfb-3882-4fd9-daf7-d4b410752b67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.15.2\" to /root/.cache/torch/hub/v0.15.2.zip\n",
            "/root/.cache/torch/hub/pytorch_vision_v0.15.2/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n",
            "<ipython-input-2-a4f1aad95ac0>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load('fomo_model.pth', map_location='cpu')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "repo = 'pytorch/vision:v0.15.2'\n",
        "model = torch.hub.load(repo, 'resnet18', weights=None)\n",
        "model.fc = nn.Linear(512, 4)\n",
        "\n",
        "state = torch.load('fomo_model.pth', map_location='cpu')\n",
        "model.load_state_dict(state)"
      ],
      "id": "YlYk-bqXfIBB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f58045c"
      },
      "source": [
        "### 11.3.1 Model Archiver\n",
        "\n",
        "Let's start with the model archive (`.mar`) file, a collection of files and folders zipped together that contains:\n",
        "- a `MAR-INF` folder with a `MANIFEST.json` file inside that describes the contents of the model archive itself, such as model and archiver versions, and the files that make up the archive\n",
        "- a serialized file containing the model's weights/state (`--serialized-file` argument)\n",
        "- a Python file containing only one class definition of our model's class inherited from `nn.Module` (only required if the model isn't scripted - more on that later) (`--model-file` argument)\n",
        "- an optional Python file containing one class definition of the handler's class inherited from `ts.torch_handler.BaseHandler` that performs the necessary transformations for pre- and post-processing  OR the name of a predefined handler (`--handler` argument)\n",
        "- an optional extra file `index_to_name.json` for mapping predicted class indices to its corresponding category names (automatically used by some predefined handlers) (`--extra-files` argument)\n",
        "\n",
        "It is typical to assemble the model archive file through the command line interface:\n",
        "\n",
        "```\n",
        "torch-model-archiver --model-name <your_model_name> \\\n",
        "                     --version <your_model_version> \\\n",
        "                     --model-file <your_model_file>.py \\\n",
        "                     --serialized-file <your_model_name>.pth \\\n",
        "                     --handler <handler-script OR name> \\\n",
        "                     --extra-files ./index_to_name.json\n",
        "```\n",
        "\n",
        "However, let's take a closer look at each one of its components and assemble it ourselves instead."
      ],
      "id": "7f58045c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dfd4a88"
      },
      "source": [
        "### 11.3.2 Model File\n",
        "\n",
        "We need to:\n",
        "- define our own class\n",
        "- create an instance of an untrained ResNet18 model\n",
        "- replace its head (`fc` layer) with our own\n",
        "- update our own class internal dictionary with the entries from ResNet's dictionary\n",
        "- set ResNet's forward pass to our own class using `setattr`\n",
        "\n",
        "It looks like this:"
      ],
      "id": "1dfd4a88"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cbe811f3"
      },
      "outputs": [],
      "source": [
        "# Creates a wrapper of FOMONet ( resnet dictionary )\n",
        "\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "class FOMONet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create an instance of an untrained ResNet18\n",
        "        resnet = resnet18(weights=None)\n",
        "        # Modifies the architecture to our task\n",
        "        resnet.fc = nn.Linear(512, 4)\n",
        "\n",
        "        # Replicate ResNet's modified architecture to FOMONet\n",
        "        self.__dict__.update(resnet.__dict__)\n",
        "        # Replicate Resnet's forward method to FOMONet\n",
        "        setattr(self, 'forward', resnet.forward)"
      ],
      "id": "cbe811f3"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b17dee7e",
        "outputId": "6f58e5b8-d8c2-4456-8e3f-c65599b07f4f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "fomo = FOMONet()\n",
        "fomo.load_state_dict(model.state_dict())"
      ],
      "id": "b17dee7e"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eed5b52c",
        "outputId": "6ac5d6ab-96b2-4c8a-e6d0-81c640cd291b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.2412, -2.8556, -1.1869,  0.8597]], grad_fn=<AddmmBackward0>),\n",
              " tensor([[ 0.2412, -2.8556, -1.1869,  0.8597]], grad_fn=<AddmmBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "fomo.eval()\n",
        "model.eval()\n",
        "\n",
        "torch.manual_seed(32)\n",
        "x = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "fomo(x), model.cpu()(x)"
      ],
      "id": "eed5b52c"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "75308290"
      },
      "outputs": [],
      "source": [
        "model_file_script = \"\"\"\n",
        "import torch.nn as nn\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "class FOMONet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create an instance of an untrained ResNet18\n",
        "        resnet = resnet18(weights=None)\n",
        "        # Modifies the architecture to our task\n",
        "        resnet.fc = nn.Linear(512, 4)\n",
        "\n",
        "        # Replicate ResNet's modified architecture to FOMONet\n",
        "        self.__dict__.update(resnet.__dict__)\n",
        "        # Replicate Resnet's forward method to FOMONet\n",
        "        setattr(self, 'forward', resnet.forward)\n",
        "\"\"\"\n",
        "\n",
        "with open('model_file.py', 'w') as fp:\n",
        "    fp.write(model_file_script)"
      ],
      "id": "75308290"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4042419b"
      },
      "source": [
        "### 11.3.3 Scripted Models\n",
        "\n",
        "\"*TorchScript is a way to create serializable and optimizable models from PyTorch code. Any TorchScript program can be saved from a Python process and loaded in a process where there is no Python dependency.*\"\n",
        "\n",
        "Source: [Torchscript](https://pytorch.org/docs/stable/jit.html)\n",
        "\n",
        "The key element here is \"*no Python dependency*\", meaning the model can be run in a standalone C++ program, for example. This preserves the best of both worlds: the ease and friendliness of the Python language for development, and the speed and reliability of the C++ language for deploying in production."
      ],
      "id": "4042419b"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3e7631af"
      },
      "outputs": [],
      "source": [
        "# once it is scripted, there is no need for the model class def anymore\n",
        "scripted_model = torch.jit.script(model)"
      ],
      "id": "3e7631af"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af97c6fa"
      },
      "source": [
        "### 11.3.4 Serialized File"
      ],
      "id": "af97c6fa"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5151de67"
      },
      "outputs": [],
      "source": [
        "# We already saved the model to disk in the previous chapter\n",
        "# eager mode version\n",
        "torch.save(model.state_dict(), 'fomo_model.pth')\n",
        "\n",
        "# scripted version\n",
        "scripted_model.save(\"fomo_model.pt\")"
      ],
      "id": "5151de67"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfb9e487"
      },
      "source": [
        "### 11.3.5 Inference Handler\n",
        "\n",
        "There are several implemented [default handlers](https://pytorch.org/serve/default_handlers.html) in Torchserve:\n",
        "- `image_classifier`\n",
        "- `object_detector`\n",
        "- `text_classifier`\n",
        "- `image_segmenter`\n",
        "\n",
        "The first three handles also implement mapping the predicted class to its corresponding names/categories using an standard `index_to_name.json` extra file."
      ],
      "id": "bfb9e487"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3848245e"
      },
      "source": [
        "#### 11.3.5.1 Initialize\n",
        "\n",
        "```python\n",
        "def initialize(self, context):\n",
        "    \"\"\"Initialize function loads the model.pt file and initialized the model object.\n",
        "       First try to load torchscript else load eager mode state_dict based model.\n",
        "    \"\"\"\n",
        "    model_file = self.manifest[\"model\"].get(\"modelFile\", \"\")\n",
        "    if model_file:\n",
        "        self.model = self._load_pickled_model(model_dir, model_file, self.model_pt_path)\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "    elif self.model_pt_path.endswith(\".pt\"):\n",
        "        self.model = self._load_torchscript_model(self.model_pt_path)\n",
        "        self.model.eval()\n",
        "```"
      ],
      "id": "3848245e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14be43e5"
      },
      "source": [
        "#### 11.3.5.2 Handle\n",
        "\n",
        "```python\n",
        "def handle(self, data, context):\n",
        "    \"\"\"Entry point for default handler. It takes the data from the input request and returns\n",
        "       the predicted outcome for the input.\n",
        "    \"\"\"\n",
        "    data_preprocess = self.preprocess(data)\n",
        "    output = self.inference(data_preprocess)\n",
        "    output = self.postprocess(output)\n",
        "\n",
        "    return output\n",
        "```"
      ],
      "id": "14be43e5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c378c5d"
      },
      "source": [
        "#### 11.3.5.3 Preprocess\n",
        "\n",
        "```python\n",
        "def preprocess(self, data):\n",
        "    \"\"\"\n",
        "    Preprocess function to convert the request input to a tensor(Torchserve supported format).\n",
        "    The user needs to override to customize the pre-processing\n",
        "    \"\"\"\n",
        "    images = []\n",
        "\n",
        "    for row in data:\n",
        "        # Compat layer: normally the envelope should just return the data\n",
        "        # directly, but older versions of Torchserve didn't have envelope.\n",
        "        image = row.get(\"data\") or row.get(\"body\")\n",
        "        if isinstance(image, str):\n",
        "            # if the image is a string of bytesarray.\n",
        "            image = base64.b64decode(image)\n",
        "\n",
        "        # If the image is sent as bytesarray\n",
        "        if isinstance(image, (bytearray, bytes)):\n",
        "            image = Image.open(io.BytesIO(image))\n",
        "            image = self.image_processing(image)\n",
        "        else:\n",
        "            # if the image is a list\n",
        "            image = torch.FloatTensor(image)\n",
        "\n",
        "        images.append(image)\n",
        "\n",
        "    return torch.stack(images).to(self.device)\n",
        "```"
      ],
      "id": "4c378c5d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a0cdd07"
      },
      "source": [
        "Let's take a quick look at the `image_processing()` function that's called by the `preprocess()` method:"
      ],
      "id": "3a0cdd07"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchserve captum"
      ],
      "metadata": {
        "id": "Skk95GlX8ii5",
        "outputId": "a814c549-12a0-41cc-ef05-4776262cc913",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Skk95GlX8ii5",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchserve in /usr/local/lib/python3.11/dist-packages (0.12.0)\n",
            "Collecting captum\n",
            "  Downloading captum-0.7.0-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from torchserve) (11.1.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from torchserve) (5.9.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from torchserve) (24.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from torchserve) (0.45.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from captum) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from captum) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.11/dist-packages (from captum) (2.5.1+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from captum) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.6->captum)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.6->captum)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.6->captum)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.6->captum)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.6->captum)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.6->captum)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.6->captum)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.6->captum)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.6->captum)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.6->captum)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6->captum) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6->captum) (3.0.2)\n",
            "Downloading captum-0.7.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, captum\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed captum-0.7.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e67e050",
        "outputId": "5b1ba772-2069-431b-becd-1591d165edbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:ts.torch_handler.base_handler:OpenVINO is not enabled\n",
            "WARNING:ts.torch_handler.base_handler:proceeding without onnxruntime\n",
            "WARNING:ts.torch_handler.base_handler:Torch TensorRT not enabled\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Compose(\n",
              "    Resize(size=256, interpolation=bilinear, max_size=None, antialias=warn)\n",
              "    CenterCrop(size=(224, 224))\n",
              "    ToTensor()\n",
              "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "from ts.torch_handler.image_classifier import ImageClassifier\n",
        "\n",
        "ImageClassifier.image_processing"
      ],
      "id": "5e67e050"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a427b9e",
        "outputId": "946c7186-ba1f-491d-dab1-555343142cbe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ImageClassification(\n",
              "    crop_size=[224]\n",
              "    resize_size=[256]\n",
              "    mean=[0.485, 0.456, 0.406]\n",
              "    std=[0.229, 0.224, 0.225]\n",
              "    interpolation=InterpolationMode.BILINEAR\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "from torchvision.models import get_weight\n",
        "\n",
        "weights = get_weight('ResNet18_Weights.DEFAULT')\n",
        "weights.transforms()"
      ],
      "id": "6a427b9e"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "MmRWcNYLyttW",
        "outputId": "6ae7a5a9-d877-4f55-ac06-143ad73b0c03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-12 15:29:45--  https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch9/fig_0_100.jpg\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4106 (4.0K) [image/jpeg]\n",
            "Saving to: ‘fig_0_100.jpg’\n",
            "\n",
            "\rfig_0_100.jpg         0%[                    ]       0  --.-KB/s               \rfig_0_100.jpg       100%[===================>]   4.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-03-12 15:29:45 (38.1 MB/s) - ‘fig_0_100.jpg’ saved [4106/4106]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/dvgodoy/assets/main/PyTorchInPractice/images/ch9/fig_0_100.jpg"
      ],
      "id": "MmRWcNYLyttW"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8f01145",
        "outputId": "ce08894d-cbcd-4319-8ab8-63317e0ca616"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(True)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "from PIL import Image\n",
        "\n",
        "img = Image.open('./fig_0_100.jpg')\n",
        "\n",
        "(ImageClassifier.image_processing(img) == weights.transforms()(img)).all()"
      ],
      "id": "d8f01145"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f2de09c"
      },
      "source": [
        "#### 11.3.5.4 Inference\n",
        "\n",
        "```python\n",
        "def inference(self, data, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    The Inference Function is used to make a prediction call on the given input request.\n",
        "    The user needs to override the inference function to customize it.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        marshalled_data = data.to(self.device)\n",
        "        results = self.model(marshalled_data, *args, **kwargs)\n",
        "    return results\n",
        "```"
      ],
      "id": "1f2de09c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86477c8b"
      },
      "source": [
        "#### 11.3.5.5 Postprocess\n",
        "\n",
        "```python\n",
        "def postprocess(self, data):\n",
        "    \"\"\"\n",
        "    The post process function makes use of the output from the inference and converts into a\n",
        "    Torchserve supported response output.\n",
        "    \"\"\"\n",
        "    ps = F.softmax(data, dim=1)\n",
        "    probs, classes = torch.topk(ps, self.topk, dim=1)\n",
        "    probs = probs.tolist()\n",
        "    classes = classes.tolist()\n",
        "    return map_class_to_label(probs, self.mapping, classes)\n",
        "```"
      ],
      "id": "86477c8b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0ee61f2"
      },
      "source": [
        "#### 11.3.5.6 Custom Handler"
      ],
      "id": "f0ee61f2"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "815d7cf5"
      },
      "outputs": [],
      "source": [
        "handler_file_script = \"\"\"\n",
        "from ts.torch_handler.image_classifier import ImageClassifier\n",
        "\n",
        "class FOMOHandler(ImageClassifier):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "      # By default, ImageClassifier uses top-5 classes\n",
        "      # but our task has only 4, so we need to tweak it\n",
        "      self.set_max_result_classes(4)\n",
        "\"\"\"\n",
        "\n",
        "with open('handler_file.py', 'w') as fp:\n",
        "    fp.write(handler_file_script)"
      ],
      "id": "815d7cf5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd336550"
      },
      "source": [
        "### 11.3.6 Extra Files"
      ],
      "id": "fd336550"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "6e3cf975"
      },
      "outputs": [],
      "source": [
        "# We didn't load the dataset in this chapter, so we're building the dict manually\n",
        "# class_to_idx = datasets['train'].class_to_idx\n",
        "\n",
        "class_to_idx = {'Fig': 0, 'Mandarine': 1, 'Onion White': 2, 'Orange': 3}"
      ],
      "id": "6e3cf975"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86f0dab9",
        "outputId": "e72bd9b0-e280-4217-a252-68c7a063fa3c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'Fig', 1: 'Mandarine', 2: 'Onion White', 3: 'Orange'}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "index_to_name = {v: k for k, v in class_to_idx.items()}\n",
        "index_to_name"
      ],
      "id": "86f0dab9"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "b3883ada"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('index_to_name.json', 'w') as f:\n",
        "    json.dump(index_to_name, f)"
      ],
      "id": "b3883ada"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab4ab790"
      },
      "source": [
        "### 11.3.7 Packaging\n",
        "\n",
        "```\n",
        "torch-model-archiver --model-name FOMO> \\\n",
        "                     --version 1.0 \\\n",
        "                     --model-file ./model_file.py \\\n",
        "                     --serialized-file fomo_model.pth \\\n",
        "                     --handler ./handler_file.py \\\n",
        "                     --extra-files ./index_to_name.json\n",
        "```"
      ],
      "id": "ab4ab790"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Y5_jJT25FUzk"
      },
      "outputs": [],
      "source": [
        "!mkdir ./model_store"
      ],
      "id": "Y5_jJT25FUzk"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-model-archiver"
      ],
      "metadata": {
        "id": "x3H9nqDm_KdI",
        "outputId": "9d4d666e-4ff7-4f7b-8efb-c178498cf3f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "x3H9nqDm_KdI",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-model-archiver\n",
            "  Downloading torch_model_archiver-0.12.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting enum-compat (from torch-model-archiver)\n",
            "  Downloading enum_compat-0.0.3-py3-none-any.whl.metadata (954 bytes)\n",
            "Downloading torch_model_archiver-0.12.0-py3-none-any.whl (16 kB)\n",
            "Downloading enum_compat-0.0.3-py3-none-any.whl (1.3 kB)\n",
            "Installing collected packages: enum-compat, torch-model-archiver\n",
            "Successfully installed enum-compat-0.0.3 torch-model-archiver-0.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "044917c6"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from model_archiver.model_packaging import generate_model_archive\n",
        "\n",
        "sys.argv = ['',\n",
        "            '--model-name', 'FOMO',\n",
        "            '--version', '1.0',\n",
        "            '--model-file', 'model_file.py',\n",
        "            '--serialized-file', 'fomo_model.pth',\n",
        "            '--handler', 'handler_file.py',\n",
        "            '--extra-files', 'index_to_name.json',\n",
        "            '--export-path', './model_store',\n",
        "            '--force']\n",
        "\n",
        "generate_model_archive()"
      ],
      "id": "044917c6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9beb18f5"
      },
      "source": [
        "## 11.4 TorchServe\n",
        "\n",
        "[TorchServe](https://pytorch.org/serve/) is a flexible and easy to use tool for serving and scaling PyTorch eager mode and scripted models in production. It offers APIs for querying, managing, and analyzing the performance of its served models (by default, they are only accessible from localhost):\n",
        "\n",
        "- [Inference API](https://github.com/pytorch/serve/blob/master/docs/inference_api.md): it listens to port 8080, and it offers the following services\n",
        "  - description (`OPTIONS /`)\n",
        "  - health check (`GET /ping`)\n",
        "  - predictions (`POST {/predictions/{model_name}`)\n",
        "  - explanations (`POST /explanations/{model_name}`)\n",
        "  - kserve (`/v1/models/{model_name}:predict:`)\n",
        "  - kserve explanations (`/v1/models/{model_name}:explain:`)\n",
        "  \n",
        "- [Management API](https://github.com/pytorch/serve/blob/master/docs/management_api.md): it listens to port 8081, and it offers the following services\n",
        "  - description (`OPTIONS /`)\n",
        "  - list models (`GET /models`)\n",
        "  - describe a model (`GET /models/{model_name}`)\n",
        "  - register a model (`POST /models`)\n",
        "  - scale workers (`POST /models/{model_name}`)\n",
        "  - set default version (`PUT /models/{model_name}/{version}/set-default`)\n",
        "  - unregister a model (`DELETE /models/{model_name}/{version}`)\n",
        "  \n",
        "- [Metrics API](https://github.com/pytorch/serve/blob/master/docs/metrics_api.md): it listens to port 8082, and it returns Prometheus-formatted frontend and backend metrics, such as number of requests, CPU and memory utilization, handler and prediction time, and many more.\n",
        "\n",
        "```\n",
        "torchserve --start \\\n",
        "           --disable-token-auth \\\n",
        "           --model-store ./model_store \\\n",
        "           --models fomo=FOMO.mar \\\n",
        "           --ts-config config.properties\n",
        "```"
      ],
      "id": "9beb18f5"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "06345f96"
      },
      "outputs": [],
      "source": [
        "config_properties = \"\"\"\n",
        "inference_address=http://127.0.0.1:7777\n",
        "\"\"\"\n",
        "\n",
        "with open('config.properties', 'w') as fp:\n",
        "    fp.write(config_properties)"
      ],
      "id": "06345f96"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "b7f6429f"
      },
      "outputs": [],
      "source": [
        "from ts.model_server import start\n",
        "\n",
        "sys.argv = ['',\n",
        "            '--start',\n",
        "            '--disable-token-auth',\n",
        "            '--model-store', './model_store',\n",
        "            '--models', 'fomo=FOMO.mar',\n",
        "            '--ts-config', 'config.properties']\n",
        "start()"
      ],
      "id": "b7f6429f"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e458c243",
        "outputId": "a4d89a4f-a348-489b-8bfe-83815a9d0821"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Fig': 0.9928925037384033,\n",
              " 'Orange': 0.004503952339291573,\n",
              " 'Onion White': 0.0016829799860715866,\n",
              " 'Mandarine': 0.0009206130634993315}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "with open('./fig_0_100.jpg', 'rb') as f:\n",
        "    data = f.read()\n",
        "\n",
        "response = requests.put('http://127.0.0.1:7777/predictions/fomo', data=data)\n",
        "response.json()"
      ],
      "id": "e458c243"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f64a41a",
        "outputId": "b3a21e7a-7b81-45e8-f54a-ebcb96c065e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TorchServe has stopped.\n"
          ]
        }
      ],
      "source": [
        "#!torchserve --stop\n",
        "sys.argv = ['', '--stop']\n",
        "start()"
      ],
      "id": "8f64a41a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cb4111d"
      },
      "source": [
        "### 11.4.1 Ngrok (optional)\n",
        "\n",
        "\"*Online in One Line*\" reads the [ngrok](https://ngrok.com/) website. It is an easy and convenient way of serving your model through a tunnel, thus allowing it to handle incoming requests from the outside world in your own Jupyter Notebook.\n",
        "\n",
        "***\n",
        "**DISCLAIMER**: You should NOT use Google Colab notebooks as backend for your deployed models. This is just a proof-of-concept, and a way to make your model available to the world for a brief amount of time, so you can showcase it to your family, friends, or colleagues.\n",
        "***\n",
        "\n",
        "If you want to try the code below, you'll need to [signup](https://dashboard.ngrok.com/signup) for a free account on [ngrok](https://ngrok.com/) and, once you're done, you can install the [pyngrok](https://pypi.org/project/pyngrok/) package that takes care of downloading and installing ngrok:"
      ],
      "id": "4cb4111d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6811ea0"
      },
      "source": [
        "You'll need to copy your [authorization token](https://dashboard.ngrok.com/get-started/your-authtoken) and paste it in the appropriate command below:\n",
        "\n",
        "***\n",
        "**DISCLAIMER**: The responsibility for keeping your credentials and/or authorization tokens safe and private is your own. Make sure to remove any credentials and/or authorizations tokens from your notebook before saving or pushing it to public repositories, such as GitHub.\n",
        "***"
      ],
      "id": "b6811ea0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f621f45",
        "outputId": "a950a5e0-1a74-4283-914d-c13c01a06f8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "# Option 1\n",
        "# You can call ngrok with your token\n",
        "# Uncomment the line below and replace ... with your token\n",
        "# !ngrok authtoken ...\n",
        "\n",
        "# Option 2\n",
        "# Or you can save it to a configuration file\n",
        "# Uncomment the line below and replace ... with your token\n",
        "# !echo \"authtoken: ...\" >> /root/.ngrok2/ngrok.yml"
      ],
      "id": "5f621f45"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a78bdddb"
      },
      "source": [
        "Once ngrok is setup, let's start Torchserve once again with a few modifications in the `config.properties` file:\n",
        "\n",
        "***\n",
        "**DISCLAIMER**: CORS stands for cross-origin resource sharing, and the configuration below makes Torchserve wide open to requests from anywhere. You SHOULD NOT use these configuration parameters in production as they're not safe. The responsibility for ensuring the security of your application, model, and data, is your own.\n",
        "***"
      ],
      "id": "a78bdddb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef8fa355"
      },
      "outputs": [],
      "source": [
        "config_properties = \"\"\"\n",
        "inference_address=http://127.0.0.1:7777\n",
        "cors_allowed_origin=*\n",
        "cors_allowed_methods=GET, POST, PUT, OPTIONS\n",
        "\"\"\"\n",
        "\n",
        "with open('config_cors.properties', 'w') as fp:\n",
        "    fp.write(config_properties)"
      ],
      "id": "ef8fa355"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f57fa8c"
      },
      "outputs": [],
      "source": [
        "sys.argv = ['',\n",
        "            '--start',\n",
        "            '--model-store', './model_store',\n",
        "            '--models', 'fomo=FOMO.mar',\n",
        "            '--ts-config', 'config_cors.properties']\n",
        "start()"
      ],
      "id": "2f57fa8c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bc063d6",
        "outputId": "f43f3562-5934-4c89-b2e7-ae660d4fb919"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-07-02T12:14:34+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# <NgrokTunnel: \"http://<public_sub>.ngrok.io\" -> \"http://localhost:7777\">\n",
        "http_tunnel = ngrok.connect(7777, \"http\")"
      ],
      "id": "4bc063d6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "e3014e2b",
        "outputId": "bfcb68d1-b71f-495f-a97d-3ce8385e24fc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://f295-35-202-252-169.ngrok-free.app'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "http_tunnel.public_url"
      ],
      "id": "e3014e2b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5118d4aa",
        "outputId": "9bd3fb2e-eeab-408d-8893-831474567311"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Fig': 0.9934685230255127,\n",
              " 'Orange': 0.004324017558246851,\n",
              " 'Onion White': 0.0012627042597159743,\n",
              " 'Mandarine': 0.0009447108022868633}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open('./fig_0_100.jpg', 'rb') as f:\n",
        "    data = f.read()\n",
        "\n",
        "response = requests.put(f'{http_tunnel.public_url}/predictions/fomo', data=data)\n",
        "response.json()"
      ],
      "id": "5118d4aa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9fad67d"
      },
      "outputs": [],
      "source": [
        "ngrok.disconnect(http_tunnel.public_url)"
      ],
      "id": "a9fad67d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dd0aaf5",
        "outputId": "555412ca-3dbc-45a9-8bd7-1ed8be392851"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TorchServe has stopped.\n"
          ]
        }
      ],
      "source": [
        "sys.argv = ['', '--stop']\n",
        "start()"
      ],
      "id": "3dd0aaf5"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}